Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

################### Loading pretrained weights from file  /nvmescratch/ceib/Prostate/nnUnet/nnUNet_results/Dataset013_PicaiP158/nnUNetTrainer_100epochs__nnUNetPlans__3d_fullres/fold_0/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 1, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 1, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 1, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 1, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 1, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 1, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 1, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 1, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.6.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.6.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 1, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 1, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([320])
decoder.stages.1.convs.0.norm.weight shape torch.Size([320])
decoder.stages.1.convs.0.norm.bias shape torch.Size([320])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([320])
decoder.stages.1.convs.1.norm.weight shape torch.Size([320])
decoder.stages.1.convs.1.norm.bias shape torch.Size([320])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.2.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([256])
decoder.stages.2.convs.0.norm.weight shape torch.Size([256])
decoder.stages.2.convs.0.norm.bias shape torch.Size([256])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([256])
decoder.stages.2.convs.1.norm.weight shape torch.Size([256])
decoder.stages.2.convs.1.norm.bias shape torch.Size([256])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.3.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([128])
decoder.stages.3.convs.0.norm.weight shape torch.Size([128])
decoder.stages.3.convs.0.norm.bias shape torch.Size([128])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([128])
decoder.stages.3.convs.1.norm.weight shape torch.Size([128])
decoder.stages.3.convs.1.norm.bias shape torch.Size([128])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.4.convs.0.conv.weight shape torch.Size([64, 128, 1, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([64])
decoder.stages.4.convs.0.norm.weight shape torch.Size([64])
decoder.stages.4.convs.0.norm.bias shape torch.Size([64])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([64, 128, 1, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.1.conv.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([64])
decoder.stages.4.convs.1.norm.weight shape torch.Size([64])
decoder.stages.4.convs.1.norm.bias shape torch.Size([64])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.5.convs.0.conv.weight shape torch.Size([32, 64, 1, 3, 3])
decoder.stages.5.convs.0.conv.bias shape torch.Size([32])
decoder.stages.5.convs.0.norm.weight shape torch.Size([32])
decoder.stages.5.convs.0.norm.bias shape torch.Size([32])
decoder.stages.5.convs.0.all_modules.0.weight shape torch.Size([32, 64, 1, 3, 3])
decoder.stages.5.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.5.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.5.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.5.convs.1.conv.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.stages.5.convs.1.conv.bias shape torch.Size([32])
decoder.stages.5.convs.1.norm.weight shape torch.Size([32])
decoder.stages.5.convs.1.norm.bias shape torch.Size([32])
decoder.stages.5.convs.1.all_modules.0.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.stages.5.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.5.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.5.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([320])
decoder.transpconvs.2.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([256])
decoder.transpconvs.3.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([128])
decoder.transpconvs.4.weight shape torch.Size([128, 64, 1, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([64])
decoder.transpconvs.5.weight shape torch.Size([64, 32, 1, 2, 2])
decoder.transpconvs.5.bias shape torch.Size([32])
################### Done ###################

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 4, 'patch_size': [24, 256, 256], 'median_image_size_in_voxels': [34.0, 308.0, 308.0], 'spacing': [3.089737057685852, 0.46875, 0.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [2, 6, 6], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset014_ProstateOwn', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.299999952316284, 0.46875, 0.46875], 'original_median_shape_after_transp': [32, 308, 308], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3013.450439453125, 'mean': 670.5831298828125, 'median': 624.0, 'min': -256.0, 'percentile_00_5': 154.0, 'percentile_99_5': 1691.131591796875, 'std': 294.5803527832031}}} 

2024-02-15 09:17:06.143178: unpacking dataset...
2024-02-15 09:17:12.288184: unpacking done...
2024-02-15 09:17:12.289120: do_dummy_2d_data_aug: True
2024-02-15 09:17:12.290407: Using splits from existing split file: /nvmescratch/ceib/Prostate/nnUnet/nnUNet_preprocessed/Dataset014_ProstateOwn/splits_final.json
2024-02-15 09:17:12.290798: The split file contains 5 splits.
2024-02-15 09:17:12.290864: Desired fold for training: 2
2024-02-15 09:17:12.290921: This split has 131 training and 33 validation cases.
2024-02-15 09:17:12.301844: Unable to plot network architecture:
2024-02-15 09:17:12.302051: No module named 'hiddenlayer'
2024-02-15 09:17:12.319082: 
2024-02-15 09:17:12.319277: Epoch 0
2024-02-15 09:17:12.319599: Current learning rate: 0.01
using pin_memory on device 0
using pin_memory on device 0
2024-02-15 09:19:24.613503: train_loss -0.5851
2024-02-15 09:19:24.613776: val_loss -0.6319
2024-02-15 09:19:24.613852: Pseudo dice [0.8251]
2024-02-15 09:19:24.613947: Epoch time: 132.3 s
2024-02-15 09:19:24.614002: Yayy! New best EMA pseudo Dice: 0.8251
2024-02-15 09:19:25.973411: 
2024-02-15 09:19:25.973502: Epoch 1
2024-02-15 09:19:25.973587: Current learning rate: 0.00991
2024-02-15 09:21:21.347092: train_loss -0.69
2024-02-15 09:21:21.347354: val_loss -0.6508
2024-02-15 09:21:21.347428: Pseudo dice [0.8212]
2024-02-15 09:21:21.347504: Epoch time: 115.37 s
2024-02-15 09:21:22.524010: 
2024-02-15 09:21:22.524141: Epoch 2
2024-02-15 09:21:22.524240: Current learning rate: 0.00982
2024-02-15 09:23:18.089574: train_loss -0.7227
2024-02-15 09:23:18.089848: val_loss -0.7182
2024-02-15 09:23:18.089922: Pseudo dice [0.8578]
2024-02-15 09:23:18.090020: Epoch time: 115.57 s
2024-02-15 09:23:18.090089: Yayy! New best EMA pseudo Dice: 0.828
2024-02-15 09:23:19.750896: 
2024-02-15 09:23:19.751039: Epoch 3
2024-02-15 09:23:19.751147: Current learning rate: 0.00973
2024-02-15 09:25:15.402453: train_loss -0.7166
2024-02-15 09:25:15.402714: val_loss -0.7018
2024-02-15 09:25:15.402785: Pseudo dice [0.8548]
2024-02-15 09:25:15.402859: Epoch time: 115.65 s
2024-02-15 09:25:15.402935: Yayy! New best EMA pseudo Dice: 0.8307
2024-02-15 09:25:17.241500: 
2024-02-15 09:25:17.241625: Epoch 4
2024-02-15 09:25:17.241718: Current learning rate: 0.00964
2024-02-15 09:27:12.668805: train_loss -0.7561
2024-02-15 09:27:12.669101: val_loss -0.7246
2024-02-15 09:27:12.669180: Pseudo dice [0.8673]
2024-02-15 09:27:12.669266: Epoch time: 115.43 s
2024-02-15 09:27:12.669339: Yayy! New best EMA pseudo Dice: 0.8343
2024-02-15 09:27:14.320248: 
2024-02-15 09:27:14.320357: Epoch 5
2024-02-15 09:27:14.320452: Current learning rate: 0.00955
2024-02-15 09:29:09.861048: train_loss -0.7514
2024-02-15 09:29:09.861336: val_loss -0.7244
2024-02-15 09:29:09.861416: Pseudo dice [0.8774]
2024-02-15 09:29:09.861507: Epoch time: 115.54 s
2024-02-15 09:29:09.861561: Yayy! New best EMA pseudo Dice: 0.8386
2024-02-15 09:29:11.491395: 
2024-02-15 09:29:11.491506: Epoch 6
2024-02-15 09:29:11.491617: Current learning rate: 0.00946
2024-02-15 09:31:07.094083: train_loss -0.7642
2024-02-15 09:31:07.094435: val_loss -0.6821
2024-02-15 09:31:07.094552: Pseudo dice [0.8667]
2024-02-15 09:31:07.094677: Epoch time: 115.6 s
2024-02-15 09:31:07.094768: Yayy! New best EMA pseudo Dice: 0.8414
2024-02-15 09:31:08.735779: 
2024-02-15 09:31:08.735890: Epoch 7
2024-02-15 09:31:08.735987: Current learning rate: 0.00937
2024-02-15 09:33:04.494363: train_loss -0.7548
2024-02-15 09:33:04.494621: val_loss -0.68
2024-02-15 09:33:04.494689: Pseudo dice [0.8609]
2024-02-15 09:33:04.494765: Epoch time: 115.76 s
2024-02-15 09:33:04.494819: Yayy! New best EMA pseudo Dice: 0.8434
2024-02-15 09:33:06.129457: 
2024-02-15 09:33:06.129694: Epoch 8
2024-02-15 09:33:06.129840: Current learning rate: 0.00928
2024-02-15 09:35:01.771892: train_loss -0.756
2024-02-15 09:35:01.772192: val_loss -0.7154
2024-02-15 09:35:01.772283: Pseudo dice [0.8691]
2024-02-15 09:35:01.772381: Epoch time: 115.64 s
2024-02-15 09:35:01.772448: Yayy! New best EMA pseudo Dice: 0.846
2024-02-15 09:35:03.619077: 
2024-02-15 09:35:03.619188: Epoch 9
2024-02-15 09:35:03.619279: Current learning rate: 0.00919
2024-02-15 09:36:59.361613: train_loss -0.7523
2024-02-15 09:36:59.361964: val_loss -0.6978
2024-02-15 09:36:59.362082: Pseudo dice [0.8635]
2024-02-15 09:36:59.362195: Epoch time: 115.74 s
2024-02-15 09:36:59.362281: Yayy! New best EMA pseudo Dice: 0.8477
2024-02-15 09:37:00.994467: 
2024-02-15 09:37:00.994580: Epoch 10
2024-02-15 09:37:00.994676: Current learning rate: 0.0091
2024-02-15 09:38:56.791726: train_loss -0.7733
2024-02-15 09:38:56.791986: val_loss -0.7166
2024-02-15 09:38:56.792054: Pseudo dice [0.8732]
2024-02-15 09:38:56.792128: Epoch time: 115.8 s
2024-02-15 09:38:56.792187: Yayy! New best EMA pseudo Dice: 0.8503
2024-02-15 09:38:58.395861: 
2024-02-15 09:38:58.395982: Epoch 11
2024-02-15 09:38:58.396078: Current learning rate: 0.009
2024-02-15 09:40:54.165029: train_loss -0.7714
2024-02-15 09:40:54.165336: val_loss -0.7523
2024-02-15 09:40:54.165405: Pseudo dice [0.8893]
2024-02-15 09:40:54.165479: Epoch time: 115.77 s
2024-02-15 09:40:54.165540: Yayy! New best EMA pseudo Dice: 0.8542
2024-02-15 09:40:55.782539: 
2024-02-15 09:40:55.782653: Epoch 12
2024-02-15 09:40:55.782749: Current learning rate: 0.00891
2024-02-15 09:42:51.562962: train_loss -0.775
2024-02-15 09:42:51.563242: val_loss -0.6352
2024-02-15 09:42:51.563314: Pseudo dice [0.8531]
2024-02-15 09:42:51.563393: Epoch time: 115.78 s
2024-02-15 09:42:52.779635: 
2024-02-15 09:42:52.779800: Epoch 13
2024-02-15 09:42:52.780076: Current learning rate: 0.00882
2024-02-15 09:44:48.538087: train_loss -0.7822
2024-02-15 09:44:48.538350: val_loss -0.7104
2024-02-15 09:44:48.538417: Pseudo dice [0.8741]
2024-02-15 09:44:48.538504: Epoch time: 115.76 s
2024-02-15 09:44:48.538567: Yayy! New best EMA pseudo Dice: 0.8561
2024-02-15 09:44:50.198571: 
2024-02-15 09:44:50.198682: Epoch 14
2024-02-15 09:44:50.198778: Current learning rate: 0.00873
2024-02-15 09:46:45.953435: train_loss -0.7784
2024-02-15 09:46:45.953784: val_loss -0.7209
2024-02-15 09:46:45.953901: Pseudo dice [0.8753]
2024-02-15 09:46:45.954031: Epoch time: 115.76 s
2024-02-15 09:46:45.954121: Yayy! New best EMA pseudo Dice: 0.858
2024-02-15 09:46:47.852849: 
2024-02-15 09:46:47.852971: Epoch 15
2024-02-15 09:46:47.853068: Current learning rate: 0.00864
2024-02-15 09:48:43.851785: train_loss -0.7815
2024-02-15 09:48:43.852178: val_loss -0.713
2024-02-15 09:48:43.852252: Pseudo dice [0.8695]
2024-02-15 09:48:43.852334: Epoch time: 116.0 s
2024-02-15 09:48:43.852386: Yayy! New best EMA pseudo Dice: 0.8591
2024-02-15 09:48:45.627231: 
2024-02-15 09:48:45.627476: Epoch 16
2024-02-15 09:48:45.627615: Current learning rate: 0.00855
2024-02-15 09:50:41.132188: train_loss -0.7903
2024-02-15 09:50:41.132450: val_loss -0.7442
2024-02-15 09:50:41.132520: Pseudo dice [0.8824]
2024-02-15 09:50:41.132598: Epoch time: 115.51 s
2024-02-15 09:50:41.132664: Yayy! New best EMA pseudo Dice: 0.8615
2024-02-15 09:50:42.821547: 
2024-02-15 09:50:42.821650: Epoch 17
2024-02-15 09:50:42.821740: Current learning rate: 0.00846
2024-02-15 09:52:38.380196: train_loss -0.8002
2024-02-15 09:52:38.380460: val_loss -0.6975
2024-02-15 09:52:38.380525: Pseudo dice [0.8708]
2024-02-15 09:52:38.380602: Epoch time: 115.56 s
2024-02-15 09:52:38.380659: Yayy! New best EMA pseudo Dice: 0.8624
2024-02-15 09:52:40.457271: 
2024-02-15 09:52:40.457380: Epoch 18
2024-02-15 09:52:40.457477: Current learning rate: 0.00836
2024-02-15 09:54:36.231968: train_loss -0.7882
2024-02-15 09:54:36.232248: val_loss -0.7324
2024-02-15 09:54:36.232322: Pseudo dice [0.876]
2024-02-15 09:54:36.232400: Epoch time: 115.78 s
2024-02-15 09:54:36.232466: Yayy! New best EMA pseudo Dice: 0.8638
2024-02-15 09:54:37.887308: 
2024-02-15 09:54:37.887484: Epoch 19
2024-02-15 09:54:37.887652: Current learning rate: 0.00827
2024-02-15 09:56:33.671773: train_loss -0.7859
2024-02-15 09:56:33.672044: val_loss -0.684
2024-02-15 09:56:33.672119: Pseudo dice [0.8731]
2024-02-15 09:56:33.672191: Epoch time: 115.79 s
2024-02-15 09:56:33.672242: Yayy! New best EMA pseudo Dice: 0.8647
2024-02-15 09:56:35.498833: 
2024-02-15 09:56:35.498955: Epoch 20
2024-02-15 09:56:35.499050: Current learning rate: 0.00818
2024-02-15 09:58:31.108305: train_loss -0.781
2024-02-15 09:58:31.108576: val_loss -0.7226
2024-02-15 09:58:31.108645: Pseudo dice [0.8763]
2024-02-15 09:58:31.108724: Epoch time: 115.61 s
2024-02-15 09:58:31.108789: Yayy! New best EMA pseudo Dice: 0.8659
2024-02-15 09:58:32.841228: 
2024-02-15 09:58:32.841353: Epoch 21
2024-02-15 09:58:32.841457: Current learning rate: 0.00809
2024-02-15 10:00:28.571051: train_loss -0.7892
2024-02-15 10:00:28.571315: val_loss -0.7328
2024-02-15 10:00:28.571383: Pseudo dice [0.8828]
2024-02-15 10:00:28.571458: Epoch time: 115.73 s
2024-02-15 10:00:28.571512: Yayy! New best EMA pseudo Dice: 0.8676
2024-02-15 10:00:30.271451: 
2024-02-15 10:00:30.271562: Epoch 22
2024-02-15 10:00:30.271832: Current learning rate: 0.008
2024-02-15 10:02:26.096933: train_loss -0.807
2024-02-15 10:02:26.097222: val_loss -0.7699
2024-02-15 10:02:26.097290: Pseudo dice [0.8871]
2024-02-15 10:02:26.097368: Epoch time: 115.83 s
2024-02-15 10:02:26.097430: Yayy! New best EMA pseudo Dice: 0.8695
2024-02-15 10:02:27.702831: 
2024-02-15 10:02:27.702952: Epoch 23
2024-02-15 10:02:27.703048: Current learning rate: 0.0079
2024-02-15 10:04:23.609450: train_loss -0.7857
2024-02-15 10:04:23.609739: val_loss -0.745
2024-02-15 10:04:23.609807: Pseudo dice [0.8879]
2024-02-15 10:04:23.609884: Epoch time: 115.91 s
2024-02-15 10:04:23.609963: Yayy! New best EMA pseudo Dice: 0.8713
2024-02-15 10:04:25.243850: 
2024-02-15 10:04:25.244001: Epoch 24
2024-02-15 10:04:25.244112: Current learning rate: 0.00781
2024-02-15 10:06:21.083733: train_loss -0.7865
2024-02-15 10:06:21.084018: val_loss -0.7436
2024-02-15 10:06:21.084090: Pseudo dice [0.8807]
2024-02-15 10:06:21.084165: Epoch time: 115.84 s
2024-02-15 10:06:21.084227: Yayy! New best EMA pseudo Dice: 0.8723
2024-02-15 10:06:22.910394: 
2024-02-15 10:06:22.910547: Epoch 25
2024-02-15 10:06:22.910650: Current learning rate: 0.00772
2024-02-15 10:08:18.613819: train_loss -0.8089
2024-02-15 10:08:18.614124: val_loss -0.7256
2024-02-15 10:08:18.614187: Pseudo dice [0.8784]
2024-02-15 10:08:18.614256: Epoch time: 115.7 s
2024-02-15 10:08:18.614318: Yayy! New best EMA pseudo Dice: 0.8729
2024-02-15 10:08:20.370958: 
2024-02-15 10:08:20.371244: Epoch 26
2024-02-15 10:08:20.371442: Current learning rate: 0.00763
2024-02-15 10:10:16.382880: train_loss -0.811
2024-02-15 10:10:16.383147: val_loss -0.6883
2024-02-15 10:10:16.383217: Pseudo dice [0.876]
2024-02-15 10:10:16.383295: Epoch time: 116.01 s
2024-02-15 10:10:16.383360: Yayy! New best EMA pseudo Dice: 0.8732
2024-02-15 10:10:18.019495: 
2024-02-15 10:10:18.019684: Epoch 27
2024-02-15 10:10:18.019880: Current learning rate: 0.00753
2024-02-15 10:12:14.051982: train_loss -0.81
2024-02-15 10:12:14.052279: val_loss -0.6682
2024-02-15 10:12:14.052352: Pseudo dice [0.8572]
2024-02-15 10:12:14.052430: Epoch time: 116.03 s
2024-02-15 10:12:15.244741: 
2024-02-15 10:12:15.244862: Epoch 28
2024-02-15 10:12:15.244966: Current learning rate: 0.00744
2024-02-15 10:14:11.201511: train_loss -0.7983
2024-02-15 10:14:11.201778: val_loss -0.711
2024-02-15 10:14:11.201846: Pseudo dice [0.8838]
2024-02-15 10:14:11.201923: Epoch time: 115.96 s
2024-02-15 10:14:12.365368: 
2024-02-15 10:14:12.365475: Epoch 29
2024-02-15 10:14:12.365567: Current learning rate: 0.00735
2024-02-15 10:16:08.224263: train_loss -0.8115
2024-02-15 10:16:08.224536: val_loss -0.7437
2024-02-15 10:16:08.224608: Pseudo dice [0.8764]
2024-02-15 10:16:08.224687: Epoch time: 115.86 s
2024-02-15 10:16:09.589831: 
2024-02-15 10:16:09.590148: Epoch 30
2024-02-15 10:16:09.590339: Current learning rate: 0.00725
2024-02-15 10:18:05.420642: train_loss -0.7862
2024-02-15 10:18:05.420901: val_loss -0.688
2024-02-15 10:18:05.420979: Pseudo dice [0.862]
2024-02-15 10:18:05.421056: Epoch time: 115.83 s
2024-02-15 10:18:06.610541: 
2024-02-15 10:18:06.610655: Epoch 31
2024-02-15 10:18:06.610749: Current learning rate: 0.00716
2024-02-15 10:20:02.503831: train_loss -0.7948
2024-02-15 10:20:02.504097: val_loss -0.6973
2024-02-15 10:20:02.504166: Pseudo dice [0.8657]
2024-02-15 10:20:02.504240: Epoch time: 115.89 s
2024-02-15 10:20:03.690311: 
2024-02-15 10:20:03.690425: Epoch 32
2024-02-15 10:20:03.690516: Current learning rate: 0.00707
2024-02-15 10:21:59.462045: train_loss -0.8033
2024-02-15 10:21:59.462309: val_loss -0.7339
2024-02-15 10:21:59.462377: Pseudo dice [0.8882]
2024-02-15 10:21:59.462454: Epoch time: 115.77 s
2024-02-15 10:22:00.765471: 
2024-02-15 10:22:00.765588: Epoch 33
2024-02-15 10:22:00.765684: Current learning rate: 0.00697
2024-02-15 10:23:56.593721: train_loss -0.8026
2024-02-15 10:23:56.593983: val_loss -0.7088
2024-02-15 10:23:56.594051: Pseudo dice [0.8788]
2024-02-15 10:23:56.594135: Epoch time: 115.83 s
2024-02-15 10:23:56.594187: Yayy! New best EMA pseudo Dice: 0.8737
2024-02-15 10:23:58.302506: 
2024-02-15 10:23:58.302785: Epoch 34
2024-02-15 10:23:58.302960: Current learning rate: 0.00688
2024-02-15 10:25:54.041979: train_loss -0.7905
2024-02-15 10:25:54.042250: val_loss -0.6966
2024-02-15 10:25:54.042319: Pseudo dice [0.8614]
2024-02-15 10:25:54.042396: Epoch time: 115.74 s
2024-02-15 10:25:55.371805: 
2024-02-15 10:25:55.372056: Epoch 35
2024-02-15 10:25:55.372271: Current learning rate: 0.00679
2024-02-15 10:27:51.336945: train_loss -0.7974
2024-02-15 10:27:51.337203: val_loss -0.7272
2024-02-15 10:27:51.337273: Pseudo dice [0.8802]
2024-02-15 10:27:51.337350: Epoch time: 115.97 s
2024-02-15 10:27:52.736994: 
2024-02-15 10:27:52.737121: Epoch 36
2024-02-15 10:27:52.737208: Current learning rate: 0.00669
2024-02-15 10:29:48.513396: train_loss -0.805
2024-02-15 10:29:48.513644: val_loss -0.7134
2024-02-15 10:29:48.513711: Pseudo dice [0.8887]
2024-02-15 10:29:48.513788: Epoch time: 115.78 s
2024-02-15 10:29:48.513841: Yayy! New best EMA pseudo Dice: 0.8748
2024-02-15 10:29:50.179870: 
2024-02-15 10:29:50.180018: Epoch 37
2024-02-15 10:29:50.180123: Current learning rate: 0.0066
2024-02-15 10:31:47.193989: train_loss -0.8178
2024-02-15 10:31:47.194283: val_loss -0.7469
2024-02-15 10:31:47.194371: Pseudo dice [0.8932]
2024-02-15 10:31:47.194458: Epoch time: 117.02 s
2024-02-15 10:31:47.194511: Yayy! New best EMA pseudo Dice: 0.8766
2024-02-15 10:31:48.977612: 
2024-02-15 10:31:48.977734: Epoch 38
2024-02-15 10:31:48.977827: Current learning rate: 0.0065
2024-02-15 10:33:44.716358: train_loss -0.8058
2024-02-15 10:33:44.716654: val_loss -0.7324
2024-02-15 10:33:44.716729: Pseudo dice [0.8834]
2024-02-15 10:33:44.716813: Epoch time: 115.74 s
2024-02-15 10:33:44.716866: Yayy! New best EMA pseudo Dice: 0.8773
2024-02-15 10:33:46.575184: 
2024-02-15 10:33:46.575312: Epoch 39
2024-02-15 10:33:46.575410: Current learning rate: 0.00641
2024-02-15 10:35:42.335713: train_loss -0.8154
2024-02-15 10:35:42.335989: val_loss -0.7028
2024-02-15 10:35:42.336059: Pseudo dice [0.8835]
2024-02-15 10:35:42.336138: Epoch time: 115.76 s
2024-02-15 10:35:42.336205: Yayy! New best EMA pseudo Dice: 0.8779
2024-02-15 10:35:44.006837: 
2024-02-15 10:35:44.007091: Epoch 40
2024-02-15 10:35:44.007273: Current learning rate: 0.00631
2024-02-15 10:37:40.038708: train_loss -0.8242
2024-02-15 10:37:40.038989: val_loss -0.7037
2024-02-15 10:37:40.039054: Pseudo dice [0.8783]
2024-02-15 10:37:40.039133: Epoch time: 116.03 s
2024-02-15 10:37:40.039198: Yayy! New best EMA pseudo Dice: 0.878
2024-02-15 10:37:42.638032: 
2024-02-15 10:37:42.638155: Epoch 41
2024-02-15 10:37:42.638257: Current learning rate: 0.00622
2024-02-15 10:39:38.628391: train_loss -0.816
2024-02-15 10:39:38.628652: val_loss -0.7543
2024-02-15 10:39:38.628718: Pseudo dice [0.8873]
2024-02-15 10:39:38.628790: Epoch time: 115.99 s
2024-02-15 10:39:38.628847: Yayy! New best EMA pseudo Dice: 0.8789
2024-02-15 10:39:40.254420: 
2024-02-15 10:39:40.254539: Epoch 42
2024-02-15 10:39:40.254630: Current learning rate: 0.00612
2024-02-15 10:41:36.288351: train_loss -0.8304
2024-02-15 10:41:36.288629: val_loss -0.726
2024-02-15 10:41:36.288697: Pseudo dice [0.8855]
2024-02-15 10:41:36.288772: Epoch time: 116.03 s
2024-02-15 10:41:36.288835: Yayy! New best EMA pseudo Dice: 0.8796
2024-02-15 10:41:37.943107: 
2024-02-15 10:41:37.943226: Epoch 43
2024-02-15 10:41:37.943319: Current learning rate: 0.00603
2024-02-15 10:43:34.127141: train_loss -0.8324
2024-02-15 10:43:34.127393: val_loss -0.7308
2024-02-15 10:43:34.127460: Pseudo dice [0.8785]
2024-02-15 10:43:34.127535: Epoch time: 116.18 s
2024-02-15 10:43:35.283521: 
2024-02-15 10:43:35.283635: Epoch 44
2024-02-15 10:43:35.283727: Current learning rate: 0.00593
2024-02-15 10:45:31.287264: train_loss -0.8292
2024-02-15 10:45:31.287525: val_loss -0.7354
2024-02-15 10:45:31.287590: Pseudo dice [0.8855]
2024-02-15 10:45:31.287661: Epoch time: 116.0 s
2024-02-15 10:45:31.287716: Yayy! New best EMA pseudo Dice: 0.8801
2024-02-15 10:45:32.988343: 
2024-02-15 10:45:32.988568: Epoch 45
2024-02-15 10:45:32.988973: Current learning rate: 0.00584
2024-02-15 10:47:28.944949: train_loss -0.8251
2024-02-15 10:47:28.945304: val_loss -0.7478
2024-02-15 10:47:28.945424: Pseudo dice [0.8807]
2024-02-15 10:47:28.945543: Epoch time: 115.96 s
2024-02-15 10:47:28.945636: Yayy! New best EMA pseudo Dice: 0.8801
2024-02-15 10:47:30.619463: 
2024-02-15 10:47:30.619576: Epoch 46
2024-02-15 10:47:30.619668: Current learning rate: 0.00574
2024-02-15 10:49:26.614440: train_loss -0.8351
2024-02-15 10:49:26.614709: val_loss -0.7135
2024-02-15 10:49:26.614783: Pseudo dice [0.8816]
2024-02-15 10:49:26.614856: Epoch time: 116.0 s
2024-02-15 10:49:26.614943: Yayy! New best EMA pseudo Dice: 0.8803
2024-02-15 10:49:28.497067: 
2024-02-15 10:49:28.497291: Epoch 47
2024-02-15 10:49:28.497478: Current learning rate: 0.00565
2024-02-15 10:51:24.368499: train_loss -0.8249
2024-02-15 10:51:24.368758: val_loss -0.7014
2024-02-15 10:51:24.368831: Pseudo dice [0.8758]
2024-02-15 10:51:24.368905: Epoch time: 115.87 s
2024-02-15 10:51:25.528971: 
2024-02-15 10:51:25.529234: Epoch 48
2024-02-15 10:51:25.529461: Current learning rate: 0.00555
2024-02-15 10:53:21.514106: train_loss -0.8321
2024-02-15 10:53:21.514374: val_loss -0.7083
2024-02-15 10:53:21.514448: Pseudo dice [0.8742]
2024-02-15 10:53:21.514528: Epoch time: 115.99 s
2024-02-15 10:53:22.673189: 
2024-02-15 10:53:22.673346: Epoch 49
2024-02-15 10:53:22.673520: Current learning rate: 0.00546
2024-02-15 10:55:18.527987: train_loss -0.8428
2024-02-15 10:55:18.528251: val_loss -0.6613
2024-02-15 10:55:18.528319: Pseudo dice [0.86]
2024-02-15 10:55:18.528391: Epoch time: 115.86 s
2024-02-15 10:55:20.166975: 
2024-02-15 10:55:20.167099: Epoch 50
2024-02-15 10:55:20.167192: Current learning rate: 0.00536
2024-02-15 10:57:16.125108: train_loss -0.8375
2024-02-15 10:57:16.125388: val_loss -0.7386
2024-02-15 10:57:16.125453: Pseudo dice [0.8827]
2024-02-15 10:57:16.125531: Epoch time: 115.96 s
2024-02-15 10:57:17.313221: 
2024-02-15 10:57:17.313477: Epoch 51
2024-02-15 10:57:17.313645: Current learning rate: 0.00526
2024-02-15 10:59:13.374578: train_loss -0.8373
2024-02-15 10:59:13.374916: val_loss -0.6682
2024-02-15 10:59:13.375040: Pseudo dice [0.8647]
2024-02-15 10:59:13.375154: Epoch time: 116.06 s
2024-02-15 10:59:14.752770: 
2024-02-15 10:59:14.752883: Epoch 52
2024-02-15 10:59:14.752981: Current learning rate: 0.00517
2024-02-15 11:01:10.699832: train_loss -0.8086
2024-02-15 11:01:10.700104: val_loss -0.6847
2024-02-15 11:01:10.700171: Pseudo dice [0.8826]
2024-02-15 11:01:10.700246: Epoch time: 115.95 s
2024-02-15 11:01:11.868509: 
2024-02-15 11:01:11.868628: Epoch 53
2024-02-15 11:01:11.868723: Current learning rate: 0.00507
2024-02-15 11:03:07.978219: train_loss -0.8261
2024-02-15 11:03:07.978634: val_loss -0.7285
2024-02-15 11:03:07.978813: Pseudo dice [0.8896]
2024-02-15 11:03:07.979026: Epoch time: 116.11 s
2024-02-15 11:03:09.180282: 
2024-02-15 11:03:09.180399: Epoch 54
2024-02-15 11:03:09.180490: Current learning rate: 0.00497
2024-02-15 11:05:05.034178: train_loss -0.8102
2024-02-15 11:05:05.034452: val_loss -0.7073
2024-02-15 11:05:05.034520: Pseudo dice [0.8921]
2024-02-15 11:05:05.034599: Epoch time: 115.85 s
2024-02-15 11:05:06.233380: 
2024-02-15 11:05:06.233492: Epoch 55
2024-02-15 11:05:06.233581: Current learning rate: 0.00487
2024-02-15 11:07:02.104235: train_loss -0.824
2024-02-15 11:07:02.104501: val_loss -0.7053
2024-02-15 11:07:02.104568: Pseudo dice [0.8842]
2024-02-15 11:07:02.104642: Epoch time: 115.87 s
2024-02-15 11:07:03.283349: 
2024-02-15 11:07:03.283587: Epoch 56
2024-02-15 11:07:03.283749: Current learning rate: 0.00478
2024-02-15 11:08:59.234036: train_loss -0.8435
2024-02-15 11:08:59.234319: val_loss -0.7231
2024-02-15 11:08:59.234396: Pseudo dice [0.8808]
2024-02-15 11:08:59.234488: Epoch time: 115.95 s
2024-02-15 11:08:59.234544: Yayy! New best EMA pseudo Dice: 0.8803
2024-02-15 11:09:00.894301: 
2024-02-15 11:09:00.894417: Epoch 57
2024-02-15 11:09:00.894567: Current learning rate: 0.00468
2024-02-15 11:10:56.809863: train_loss -0.8311
2024-02-15 11:10:56.810170: val_loss -0.7582
2024-02-15 11:10:56.810249: Pseudo dice [0.8979]
2024-02-15 11:10:56.810348: Epoch time: 115.92 s
2024-02-15 11:10:56.810404: Yayy! New best EMA pseudo Dice: 0.882
2024-02-15 11:10:58.663355: 
2024-02-15 11:10:58.663476: Epoch 58
2024-02-15 11:10:58.663585: Current learning rate: 0.00458
2024-02-15 11:12:54.636298: train_loss -0.8339
2024-02-15 11:12:54.636596: val_loss -0.6974
2024-02-15 11:12:54.636663: Pseudo dice [0.8604]
2024-02-15 11:12:54.636744: Epoch time: 115.97 s
2024-02-15 11:12:55.869855: 
2024-02-15 11:12:55.869995: Epoch 59
2024-02-15 11:12:55.870095: Current learning rate: 0.00448
2024-02-15 11:14:51.922366: train_loss -0.8363
2024-02-15 11:14:51.922624: val_loss -0.7368
2024-02-15 11:14:51.922693: Pseudo dice [0.8937]
2024-02-15 11:14:51.922767: Epoch time: 116.05 s
2024-02-15 11:14:53.170825: 
2024-02-15 11:14:53.170950: Epoch 60
2024-02-15 11:14:53.171042: Current learning rate: 0.00438
2024-02-15 11:16:49.042637: train_loss -0.8424
2024-02-15 11:16:49.042904: val_loss -0.7441
2024-02-15 11:16:49.042979: Pseudo dice [0.8832]
2024-02-15 11:16:49.043056: Epoch time: 115.87 s
2024-02-15 11:16:50.233776: 
2024-02-15 11:16:50.233910: Epoch 61
2024-02-15 11:16:50.234011: Current learning rate: 0.00429
2024-02-15 11:18:46.019107: train_loss -0.8274
2024-02-15 11:18:46.019404: val_loss -0.671
2024-02-15 11:18:46.019483: Pseudo dice [0.8584]
2024-02-15 11:18:46.019569: Epoch time: 115.79 s
2024-02-15 11:18:47.221933: 
2024-02-15 11:18:47.222043: Epoch 62
2024-02-15 11:18:47.222138: Current learning rate: 0.00419
2024-02-15 11:20:43.127493: train_loss -0.8391
2024-02-15 11:20:43.127770: val_loss -0.731
2024-02-15 11:20:43.127842: Pseudo dice [0.8823]
2024-02-15 11:20:43.127931: Epoch time: 115.91 s
2024-02-15 11:20:44.356710: 
2024-02-15 11:20:44.356849: Epoch 63
2024-02-15 11:20:44.356955: Current learning rate: 0.00409
2024-02-15 11:22:40.362311: train_loss -0.8238
2024-02-15 11:22:40.362581: val_loss -0.7347
2024-02-15 11:22:40.362650: Pseudo dice [0.8832]
2024-02-15 11:22:40.362725: Epoch time: 116.01 s
2024-02-15 11:22:41.768058: 
2024-02-15 11:22:41.768215: Epoch 64
2024-02-15 11:22:41.768421: Current learning rate: 0.00399
2024-02-15 11:24:37.876382: train_loss -0.8261
2024-02-15 11:24:37.876662: val_loss -0.7104
2024-02-15 11:24:37.876732: Pseudo dice [0.884]
2024-02-15 11:24:37.876806: Epoch time: 116.11 s
2024-02-15 11:24:39.105212: 
2024-02-15 11:24:39.105326: Epoch 65
2024-02-15 11:24:39.105418: Current learning rate: 0.00389
2024-02-15 11:26:35.021749: train_loss -0.8255
2024-02-15 11:26:35.022017: val_loss -0.7511
2024-02-15 11:26:35.022108: Pseudo dice [0.8955]
2024-02-15 11:26:35.022194: Epoch time: 115.92 s
2024-02-15 11:26:36.369576: 
2024-02-15 11:26:36.369688: Epoch 66
2024-02-15 11:26:36.369779: Current learning rate: 0.00379
2024-02-15 11:28:32.237835: train_loss -0.8168
2024-02-15 11:28:32.238108: val_loss -0.7012
2024-02-15 11:28:32.238176: Pseudo dice [0.8846]
2024-02-15 11:28:32.238250: Epoch time: 115.87 s
2024-02-15 11:28:32.238317: Yayy! New best EMA pseudo Dice: 0.8821
2024-02-15 11:28:33.887803: 
2024-02-15 11:28:33.887912: Epoch 67
2024-02-15 11:28:33.888009: Current learning rate: 0.00369
2024-02-15 11:30:29.800350: train_loss -0.8283
2024-02-15 11:30:29.800614: val_loss -0.7025
2024-02-15 11:30:29.800682: Pseudo dice [0.8651]
2024-02-15 11:30:29.800757: Epoch time: 115.91 s
2024-02-15 11:30:31.014871: 
2024-02-15 11:30:31.014985: Epoch 68
2024-02-15 11:30:31.015079: Current learning rate: 0.00359
2024-02-15 11:32:26.789002: train_loss -0.8301
2024-02-15 11:32:26.789263: val_loss -0.7174
2024-02-15 11:32:26.789334: Pseudo dice [0.8763]
2024-02-15 11:32:26.789408: Epoch time: 115.78 s
2024-02-15 11:32:28.189373: 
2024-02-15 11:32:28.189488: Epoch 69
2024-02-15 11:32:28.189578: Current learning rate: 0.00349
2024-02-15 11:34:23.890673: train_loss -0.8465
2024-02-15 11:34:23.890945: val_loss -0.731
2024-02-15 11:34:23.891016: Pseudo dice [0.8744]
2024-02-15 11:34:23.891094: Epoch time: 115.7 s
2024-02-15 11:34:25.257324: 
2024-02-15 11:34:25.257445: Epoch 70
2024-02-15 11:34:25.257744: Current learning rate: 0.00338
2024-02-15 11:36:21.175272: train_loss -0.8326
2024-02-15 11:36:21.175530: val_loss -0.728
2024-02-15 11:36:21.175592: Pseudo dice [0.8852]
2024-02-15 11:36:21.175664: Epoch time: 115.92 s
2024-02-15 11:36:22.384350: 
2024-02-15 11:36:22.384467: Epoch 71
2024-02-15 11:36:22.384560: Current learning rate: 0.00328
2024-02-15 11:38:18.223477: train_loss -0.8462
2024-02-15 11:38:18.223827: val_loss -0.6943
2024-02-15 11:38:18.223957: Pseudo dice [0.8785]
2024-02-15 11:38:18.224070: Epoch time: 115.84 s
2024-02-15 11:38:19.470296: 
2024-02-15 11:38:19.470405: Epoch 72
2024-02-15 11:38:19.470494: Current learning rate: 0.00318
2024-02-15 11:40:15.172815: train_loss -0.8415
2024-02-15 11:40:15.173239: val_loss -0.7273
2024-02-15 11:40:15.173435: Pseudo dice [0.8785]
2024-02-15 11:40:15.173600: Epoch time: 115.7 s
2024-02-15 11:40:16.391950: 
2024-02-15 11:40:16.392061: Epoch 73
2024-02-15 11:40:16.392160: Current learning rate: 0.00308
2024-02-15 11:42:12.281745: train_loss -0.8468
2024-02-15 11:42:12.282009: val_loss -0.6834
2024-02-15 11:42:12.282076: Pseudo dice [0.8705]
2024-02-15 11:42:12.282151: Epoch time: 115.89 s
2024-02-15 11:42:13.682253: 
2024-02-15 11:42:13.682377: Epoch 74
2024-02-15 11:42:13.682476: Current learning rate: 0.00297
2024-02-15 11:44:09.575207: train_loss -0.8343
2024-02-15 11:44:09.575533: val_loss -0.738
2024-02-15 11:44:09.575649: Pseudo dice [0.8938]
2024-02-15 11:44:09.575761: Epoch time: 115.89 s
2024-02-15 11:44:10.866205: 
2024-02-15 11:44:10.866324: Epoch 75
2024-02-15 11:44:10.866418: Current learning rate: 0.00287
2024-02-15 11:46:06.756558: train_loss -0.8463
2024-02-15 11:46:06.756822: val_loss -0.7057
2024-02-15 11:46:06.756889: Pseudo dice [0.8876]
2024-02-15 11:46:06.756969: Epoch time: 115.89 s
2024-02-15 11:46:07.988332: 
2024-02-15 11:46:07.988441: Epoch 76
2024-02-15 11:46:07.988533: Current learning rate: 0.00277
2024-02-15 11:48:03.933061: train_loss -0.8433
2024-02-15 11:48:03.933421: val_loss -0.6822
2024-02-15 11:48:03.933527: Pseudo dice [0.8687]
2024-02-15 11:48:03.933642: Epoch time: 115.95 s
2024-02-15 11:48:05.164091: 
2024-02-15 11:48:05.164205: Epoch 77
2024-02-15 11:48:05.164310: Current learning rate: 0.00266
2024-02-15 11:50:01.088710: train_loss -0.85
2024-02-15 11:50:01.088993: val_loss -0.6868
2024-02-15 11:50:01.089064: Pseudo dice [0.8697]
2024-02-15 11:50:01.089150: Epoch time: 115.93 s
2024-02-15 11:50:02.328491: 
2024-02-15 11:50:02.328597: Epoch 78
2024-02-15 11:50:02.328692: Current learning rate: 0.00256
2024-02-15 11:51:58.414714: train_loss -0.8556
2024-02-15 11:51:58.414990: val_loss -0.7225
2024-02-15 11:51:58.415062: Pseudo dice [0.8831]
2024-02-15 11:51:58.415162: Epoch time: 116.09 s
2024-02-15 11:51:59.884390: 
2024-02-15 11:51:59.884845: Epoch 79
2024-02-15 11:51:59.885005: Current learning rate: 0.00245
2024-02-15 11:53:55.666164: train_loss -0.8468
2024-02-15 11:53:55.666412: val_loss -0.7088
2024-02-15 11:53:55.666476: Pseudo dice [0.8787]
2024-02-15 11:53:55.666548: Epoch time: 115.78 s
2024-02-15 11:53:56.917106: 
2024-02-15 11:53:56.917222: Epoch 80
2024-02-15 11:53:56.917313: Current learning rate: 0.00235
2024-02-15 11:55:52.967367: train_loss -0.8362
2024-02-15 11:55:52.967634: val_loss -0.6766
2024-02-15 11:55:52.967705: Pseudo dice [0.8791]
2024-02-15 11:55:52.967783: Epoch time: 116.05 s
2024-02-15 11:55:54.236899: 
2024-02-15 11:55:54.237023: Epoch 81
2024-02-15 11:55:54.237120: Current learning rate: 0.00224
2024-02-15 11:57:50.232396: train_loss -0.8395
2024-02-15 11:57:50.232689: val_loss -0.7215
2024-02-15 11:57:50.232757: Pseudo dice [0.8826]
2024-02-15 11:57:50.232831: Epoch time: 116.0 s
2024-02-15 11:57:51.623332: 
2024-02-15 11:57:51.623445: Epoch 82
2024-02-15 11:57:51.623539: Current learning rate: 0.00214
2024-02-15 11:59:47.608579: train_loss -0.8473
2024-02-15 11:59:47.608828: val_loss -0.7315
2024-02-15 11:59:47.608893: Pseudo dice [0.8909]
2024-02-15 11:59:47.608969: Epoch time: 115.99 s
2024-02-15 11:59:48.768842: 
2024-02-15 11:59:48.768960: Epoch 83
2024-02-15 11:59:48.769052: Current learning rate: 0.00203
2024-02-15 12:01:44.618622: train_loss -0.8474
2024-02-15 12:01:44.618912: val_loss -0.7258
2024-02-15 12:01:44.618999: Pseudo dice [0.8845]
2024-02-15 12:01:44.619103: Epoch time: 115.85 s
2024-02-15 12:01:45.792759: 
2024-02-15 12:01:45.792872: Epoch 84
2024-02-15 12:01:45.792970: Current learning rate: 0.00192
2024-02-15 12:03:41.699356: train_loss -0.8395
2024-02-15 12:03:41.699630: val_loss -0.7445
2024-02-15 12:03:41.699704: Pseudo dice [0.8969]
2024-02-15 12:03:41.699784: Epoch time: 115.91 s
2024-02-15 12:03:41.699850: Yayy! New best EMA pseudo Dice: 0.8826
2024-02-15 12:03:43.521476: 
2024-02-15 12:03:43.521594: Epoch 85
2024-02-15 12:03:43.521683: Current learning rate: 0.00181
2024-02-15 12:05:39.463043: train_loss -0.8613
2024-02-15 12:05:39.463303: val_loss -0.7475
2024-02-15 12:05:39.463369: Pseudo dice [0.8886]
2024-02-15 12:05:39.463447: Epoch time: 115.94 s
2024-02-15 12:05:39.463510: Yayy! New best EMA pseudo Dice: 0.8832
2024-02-15 12:05:41.090656: 
2024-02-15 12:05:41.090771: Epoch 86
2024-02-15 12:05:41.090864: Current learning rate: 0.0017
2024-02-15 12:07:37.052422: train_loss -0.8576
2024-02-15 12:07:37.052962: val_loss -0.6659
2024-02-15 12:07:37.053053: Pseudo dice [0.8639]
2024-02-15 12:07:37.053143: Epoch time: 115.96 s
2024-02-15 12:07:38.286279: 
2024-02-15 12:07:38.286403: Epoch 87
2024-02-15 12:07:38.286504: Current learning rate: 0.00159
2024-02-15 12:09:33.958192: train_loss -0.8434
2024-02-15 12:09:33.958445: val_loss -0.7316
2024-02-15 12:09:33.958510: Pseudo dice [0.8925]
2024-02-15 12:09:33.958580: Epoch time: 115.67 s
2024-02-15 12:09:35.109414: 
2024-02-15 12:09:35.109530: Epoch 88
2024-02-15 12:09:35.109625: Current learning rate: 0.00148
2024-02-15 12:11:30.753855: train_loss -0.8554
2024-02-15 12:11:30.754129: val_loss -0.7651
2024-02-15 12:11:30.754197: Pseudo dice [0.8976]
2024-02-15 12:11:30.754274: Epoch time: 115.65 s
2024-02-15 12:11:30.754340: Yayy! New best EMA pseudo Dice: 0.8839
2024-02-15 12:11:32.345767: 
2024-02-15 12:11:32.345908: Epoch 89
2024-02-15 12:11:32.346014: Current learning rate: 0.00137
2024-02-15 12:13:28.417995: train_loss -0.8436
2024-02-15 12:13:28.418285: val_loss -0.7268
2024-02-15 12:13:28.418361: Pseudo dice [0.8824]
2024-02-15 12:13:28.418446: Epoch time: 116.07 s
2024-02-15 12:13:29.915899: 
2024-02-15 12:13:29.916018: Epoch 90
2024-02-15 12:13:29.916113: Current learning rate: 0.00126
2024-02-15 12:15:25.779795: train_loss -0.8492
2024-02-15 12:15:25.780074: val_loss -0.7381
2024-02-15 12:15:25.780141: Pseudo dice [0.8841]
2024-02-15 12:15:25.780217: Epoch time: 115.86 s
2024-02-15 12:15:26.938737: 
2024-02-15 12:15:26.938859: Epoch 91
2024-02-15 12:15:26.938960: Current learning rate: 0.00115
2024-02-15 12:17:22.925752: train_loss -0.8478
2024-02-15 12:17:22.926053: val_loss -0.7277
2024-02-15 12:17:22.926124: Pseudo dice [0.882]
2024-02-15 12:17:22.926211: Epoch time: 115.99 s
2024-02-15 12:17:24.081656: 
2024-02-15 12:17:24.081780: Epoch 92
2024-02-15 12:17:24.081871: Current learning rate: 0.00103
2024-02-15 12:19:19.892533: train_loss -0.8536
2024-02-15 12:19:19.892803: val_loss -0.7397
2024-02-15 12:19:19.892875: Pseudo dice [0.8836]
2024-02-15 12:19:19.892968: Epoch time: 115.81 s
2024-02-15 12:19:21.051862: 
2024-02-15 12:19:21.051986: Epoch 93
2024-02-15 12:19:21.052077: Current learning rate: 0.00091
2024-02-15 12:21:17.046017: train_loss -0.8412
2024-02-15 12:21:17.046376: val_loss -0.7373
2024-02-15 12:21:17.046489: Pseudo dice [0.8915]
2024-02-15 12:21:17.046603: Epoch time: 116.0 s
2024-02-15 12:21:17.046690: Yayy! New best EMA pseudo Dice: 0.8844
2024-02-15 12:21:18.743815: 
2024-02-15 12:21:18.743998: Epoch 94
2024-02-15 12:21:18.744275: Current learning rate: 0.00079
2024-02-15 12:23:14.668828: train_loss -0.8435
2024-02-15 12:23:14.669098: val_loss -0.7664
2024-02-15 12:23:14.669169: Pseudo dice [0.8975]
2024-02-15 12:23:14.669249: Epoch time: 115.93 s
2024-02-15 12:23:14.669319: Yayy! New best EMA pseudo Dice: 0.8857
2024-02-15 12:23:16.304784: 
2024-02-15 12:23:16.305059: Epoch 95
2024-02-15 12:23:16.305167: Current learning rate: 0.00067
2024-02-15 12:25:12.124835: train_loss -0.8578
2024-02-15 12:25:12.125142: val_loss -0.683
2024-02-15 12:25:12.125212: Pseudo dice [0.8665]
2024-02-15 12:25:12.125304: Epoch time: 115.82 s
2024-02-15 12:25:13.503279: 
2024-02-15 12:25:13.503415: Epoch 96
2024-02-15 12:25:13.503522: Current learning rate: 0.00055
2024-02-15 12:27:09.660608: train_loss -0.856
2024-02-15 12:27:09.660877: val_loss -0.6866
2024-02-15 12:27:09.660950: Pseudo dice [0.8675]
2024-02-15 12:27:09.661025: Epoch time: 116.16 s
2024-02-15 12:27:10.834895: 
2024-02-15 12:27:10.835025: Epoch 97
2024-02-15 12:27:10.835138: Current learning rate: 0.00043
2024-02-15 12:29:06.901175: train_loss -0.8488
2024-02-15 12:29:06.901462: val_loss -0.747
2024-02-15 12:29:06.901530: Pseudo dice [0.8829]
2024-02-15 12:29:06.901620: Epoch time: 116.07 s
2024-02-15 12:29:08.239774: 
2024-02-15 12:29:08.240052: Epoch 98
2024-02-15 12:29:08.240205: Current learning rate: 0.0003
2024-02-15 12:31:04.453675: train_loss -0.8565
2024-02-15 12:31:04.453969: val_loss -0.743
2024-02-15 12:31:04.454053: Pseudo dice [0.8894]
2024-02-15 12:31:04.454131: Epoch time: 116.21 s
2024-02-15 12:31:05.651780: 
2024-02-15 12:31:05.651902: Epoch 99
2024-02-15 12:31:05.652003: Current learning rate: 0.00016
2024-02-15 12:33:01.704500: train_loss -0.8501
2024-02-15 12:33:01.704784: val_loss -0.6801
2024-02-15 12:33:01.704861: Pseudo dice [0.8712]
2024-02-15 12:33:01.704948: Epoch time: 116.05 s
2024-02-15 12:33:03.338261: Training done.
2024-02-15 12:33:03.389410: Using splits from existing split file: /nvmescratch/ceib/Prostate/nnUnet/nnUNet_preprocessed/Dataset014_ProstateOwn/splits_final.json
2024-02-15 12:33:03.389857: The split file contains 5 splits.
2024-02-15 12:33:03.389941: Desired fold for training: 2
2024-02-15 12:33:03.390010: This split has 131 training and 33 validation cases.
2024-02-15 12:33:03.390441: predicting ProstateOwn_0
2024-02-15 12:33:03.391790: ProstateOwn_0, shape torch.Size([1, 68, 359, 359]), rank 0
2024-02-15 12:33:11.576746: predicting ProstateOwn_100
2024-02-15 12:33:11.579905: ProstateOwn_100, shape torch.Size([1, 33, 308, 308]), rank 0
2024-02-15 12:33:14.080467: predicting ProstateOwn_103
2024-02-15 12:33:14.081971: ProstateOwn_103, shape torch.Size([1, 43, 308, 308]), rank 0
2024-02-15 12:33:17.813250: predicting ProstateOwn_117
2024-02-15 12:33:17.815122: ProstateOwn_117, shape torch.Size([1, 56, 359, 359]), rank 0
2024-02-15 12:33:22.808551: predicting ProstateOwn_121
2024-02-15 12:33:22.811022: ProstateOwn_121, shape torch.Size([1, 38, 308, 308]), rank 0
2024-02-15 12:33:26.557863: predicting ProstateOwn_125
2024-02-15 12:33:26.559921: ProstateOwn_125, shape torch.Size([1, 34, 307, 307]), rank 0
2024-02-15 12:33:29.060808: predicting ProstateOwn_127
2024-02-15 12:33:29.062669: ProstateOwn_127, shape torch.Size([1, 31, 308, 308]), rank 0
2024-02-15 12:33:31.560083: predicting ProstateOwn_131
2024-02-15 12:33:31.562140: ProstateOwn_131, shape torch.Size([1, 80, 449, 449]), rank 0
2024-02-15 12:33:48.369848: predicting ProstateOwn_138
2024-02-15 12:33:48.373481: ProstateOwn_138, shape torch.Size([1, 34, 307, 307]), rank 0
2024-02-15 12:33:50.874548: predicting ProstateOwn_139
2024-02-15 12:33:50.876104: ProstateOwn_139, shape torch.Size([1, 77, 359, 359]), rank 0
2024-02-15 12:33:58.347363: predicting ProstateOwn_14
2024-02-15 12:33:58.350037: ProstateOwn_14, shape torch.Size([1, 35, 308, 308]), rank 0
2024-02-15 12:34:00.853935: predicting ProstateOwn_142
2024-02-15 12:34:00.855837: ProstateOwn_142, shape torch.Size([1, 74, 488, 488]), rank 0
2024-02-15 12:34:17.603806: predicting ProstateOwn_15
2024-02-15 12:34:17.607884: ProstateOwn_15, shape torch.Size([1, 60, 359, 359]), rank 0
2024-02-15 12:34:22.602420: predicting ProstateOwn_150
2024-02-15 12:34:22.604507: ProstateOwn_150, shape torch.Size([1, 77, 359, 359]), rank 0
2024-02-15 12:34:30.076745: predicting ProstateOwn_155
2024-02-15 12:34:30.079870: ProstateOwn_155, shape torch.Size([1, 67, 411, 411]), rank 0
2024-02-15 12:34:44.081425: predicting ProstateOwn_157
2024-02-15 12:34:44.084075: ProstateOwn_157, shape torch.Size([1, 36, 308, 308]), rank 0
2024-02-15 12:34:46.588625: predicting ProstateOwn_158
2024-02-15 12:34:46.591065: ProstateOwn_158, shape torch.Size([1, 34, 308, 308]), rank 0
2024-02-15 12:34:49.091618: predicting ProstateOwn_159
2024-02-15 12:34:49.093516: ProstateOwn_159, shape torch.Size([1, 44, 308, 308]), rank 0
2024-02-15 12:34:52.826898: predicting ProstateOwn_160
2024-02-15 12:34:52.828540: ProstateOwn_160, shape torch.Size([1, 26, 308, 308]), rank 0
2024-02-15 12:34:55.327507: predicting ProstateOwn_28
2024-02-15 12:34:55.328650: ProstateOwn_28, shape torch.Size([1, 35, 308, 308]), rank 0
2024-02-15 12:34:57.825059: predicting ProstateOwn_36
2024-02-15 12:34:57.827064: ProstateOwn_36, shape torch.Size([1, 44, 294, 294]), rank 0
2024-02-15 12:35:01.563279: predicting ProstateOwn_44
2024-02-15 12:35:01.565660: ProstateOwn_44, shape torch.Size([1, 28, 308, 308]), rank 0
2024-02-15 12:35:04.068388: predicting ProstateOwn_5
2024-02-15 12:35:04.070156: ProstateOwn_5, shape torch.Size([1, 33, 308, 308]), rank 0
2024-02-15 12:35:06.570224: predicting ProstateOwn_6
2024-02-15 12:35:06.571923: ProstateOwn_6, shape torch.Size([1, 27, 257, 257]), rank 0
2024-02-15 12:35:09.064603: predicting ProstateOwn_60
2024-02-15 12:35:09.066113: ProstateOwn_60, shape torch.Size([1, 28, 308, 308]), rank 0
2024-02-15 12:35:11.564451: predicting ProstateOwn_62
2024-02-15 12:35:11.566211: ProstateOwn_62, shape torch.Size([1, 44, 308, 308]), rank 0
2024-02-15 12:35:15.303747: predicting ProstateOwn_71
2024-02-15 12:35:15.305793: ProstateOwn_71, shape torch.Size([1, 31, 308, 308]), rank 0
2024-02-15 12:35:17.805754: predicting ProstateOwn_73
2024-02-15 12:35:17.807896: ProstateOwn_73, shape torch.Size([1, 67, 411, 411]), rank 0
2024-02-15 12:35:31.805943: predicting ProstateOwn_80
2024-02-15 12:35:31.809995: ProstateOwn_80, shape torch.Size([1, 34, 321, 321]), rank 0
2024-02-15 12:35:34.315022: predicting ProstateOwn_81
2024-02-15 12:35:34.317121: ProstateOwn_81, shape torch.Size([1, 26, 308, 308]), rank 0
2024-02-15 12:35:36.815641: predicting ProstateOwn_9
2024-02-15 12:35:36.817556: ProstateOwn_9, shape torch.Size([1, 34, 307, 307]), rank 0
2024-02-15 12:35:39.316633: predicting ProstateOwn_96
2024-02-15 12:35:39.318353: ProstateOwn_96, shape torch.Size([1, 29, 308, 308]), rank 0
2024-02-15 12:35:41.817175: predicting ProstateOwn_98
2024-02-15 12:35:41.819071: ProstateOwn_98, shape torch.Size([1, 29, 308, 308]), rank 0
2024-02-15 12:36:06.974127: Validation complete
2024-02-15 12:36:06.974269: Mean Validation Dice:  0.8475062439610743
