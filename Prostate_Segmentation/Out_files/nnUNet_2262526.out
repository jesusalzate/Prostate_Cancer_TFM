Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

################### Loading pretrained weights from file  /nvmescratch/ceib/Prostate/nnUnet/nnUNet_results/Dataset013_PicaiP158/nnUNetTrainer_100epochs__nnUNetPlans__3d_fullres/fold_0/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 1, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 1, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 1, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 1, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 1, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 1, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 1, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 1, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.6.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.6.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 1, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 1, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([320])
decoder.stages.1.convs.0.norm.weight shape torch.Size([320])
decoder.stages.1.convs.0.norm.bias shape torch.Size([320])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([320])
decoder.stages.1.convs.1.norm.weight shape torch.Size([320])
decoder.stages.1.convs.1.norm.bias shape torch.Size([320])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.2.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([256])
decoder.stages.2.convs.0.norm.weight shape torch.Size([256])
decoder.stages.2.convs.0.norm.bias shape torch.Size([256])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([256])
decoder.stages.2.convs.1.norm.weight shape torch.Size([256])
decoder.stages.2.convs.1.norm.bias shape torch.Size([256])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.3.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([128])
decoder.stages.3.convs.0.norm.weight shape torch.Size([128])
decoder.stages.3.convs.0.norm.bias shape torch.Size([128])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([128])
decoder.stages.3.convs.1.norm.weight shape torch.Size([128])
decoder.stages.3.convs.1.norm.bias shape torch.Size([128])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.4.convs.0.conv.weight shape torch.Size([64, 128, 1, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([64])
decoder.stages.4.convs.0.norm.weight shape torch.Size([64])
decoder.stages.4.convs.0.norm.bias shape torch.Size([64])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([64, 128, 1, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.1.conv.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([64])
decoder.stages.4.convs.1.norm.weight shape torch.Size([64])
decoder.stages.4.convs.1.norm.bias shape torch.Size([64])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([64, 64, 1, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.5.convs.0.conv.weight shape torch.Size([32, 64, 1, 3, 3])
decoder.stages.5.convs.0.conv.bias shape torch.Size([32])
decoder.stages.5.convs.0.norm.weight shape torch.Size([32])
decoder.stages.5.convs.0.norm.bias shape torch.Size([32])
decoder.stages.5.convs.0.all_modules.0.weight shape torch.Size([32, 64, 1, 3, 3])
decoder.stages.5.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.5.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.5.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.5.convs.1.conv.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.stages.5.convs.1.conv.bias shape torch.Size([32])
decoder.stages.5.convs.1.norm.weight shape torch.Size([32])
decoder.stages.5.convs.1.norm.bias shape torch.Size([32])
decoder.stages.5.convs.1.all_modules.0.weight shape torch.Size([32, 32, 1, 3, 3])
decoder.stages.5.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.5.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.5.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([320])
decoder.transpconvs.2.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([256])
decoder.transpconvs.3.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([128])
decoder.transpconvs.4.weight shape torch.Size([128, 64, 1, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([64])
decoder.transpconvs.5.weight shape torch.Size([64, 32, 1, 2, 2])
decoder.transpconvs.5.bias shape torch.Size([32])
################### Done ###################

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 4, 'patch_size': [24, 256, 256], 'median_image_size_in_voxels': [34.0, 308.0, 308.0], 'spacing': [3.089737057685852, 0.46875, 0.46875], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [2, 6, 6], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset014_ProstateOwn', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.299999952316284, 0.46875, 0.46875], 'original_median_shape_after_transp': [32, 308, 308], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3013.450439453125, 'mean': 670.5831298828125, 'median': 624.0, 'min': -256.0, 'percentile_00_5': 154.0, 'percentile_99_5': 1691.131591796875, 'std': 294.5803527832031}}} 

2024-02-15 12:36:44.600663: unpacking dataset...
2024-02-15 12:36:50.794092: unpacking done...
2024-02-15 12:36:50.795019: do_dummy_2d_data_aug: True
2024-02-15 12:36:50.796311: Using splits from existing split file: /nvmescratch/ceib/Prostate/nnUnet/nnUNet_preprocessed/Dataset014_ProstateOwn/splits_final.json
2024-02-15 12:36:50.796705: The split file contains 5 splits.
2024-02-15 12:36:50.796775: Desired fold for training: 4
2024-02-15 12:36:50.796832: This split has 132 training and 32 validation cases.
2024-02-15 12:36:50.812277: Unable to plot network architecture:
2024-02-15 12:36:50.812446: No module named 'hiddenlayer'
2024-02-15 12:36:50.826235: 
2024-02-15 12:36:50.826412: Epoch 0
2024-02-15 12:36:50.826704: Current learning rate: 0.01
using pin_memory on device 0
using pin_memory on device 0
2024-02-15 12:39:03.735907: train_loss -0.5897
2024-02-15 12:39:03.736216: val_loss -0.6179
2024-02-15 12:39:03.736298: Pseudo dice [0.7961]
2024-02-15 12:39:03.736381: Epoch time: 132.91 s
2024-02-15 12:39:03.736440: Yayy! New best EMA pseudo Dice: 0.7961
2024-02-15 12:39:05.233272: 
2024-02-15 12:39:05.233370: Epoch 1
2024-02-15 12:39:05.233462: Current learning rate: 0.00991
2024-02-15 12:41:00.740213: train_loss -0.7081
2024-02-15 12:41:00.740483: val_loss -0.7134
2024-02-15 12:41:00.740556: Pseudo dice [0.8522]
2024-02-15 12:41:00.740633: Epoch time: 115.51 s
2024-02-15 12:41:00.740699: Yayy! New best EMA pseudo Dice: 0.8017
2024-02-15 12:41:02.385359: 
2024-02-15 12:41:02.385480: Epoch 2
2024-02-15 12:41:02.385580: Current learning rate: 0.00982
2024-02-15 12:42:58.099651: train_loss -0.7152
2024-02-15 12:42:58.099917: val_loss -0.6866
2024-02-15 12:42:58.100010: Pseudo dice [0.8583]
2024-02-15 12:42:58.100092: Epoch time: 115.72 s
2024-02-15 12:42:58.100171: Yayy! New best EMA pseudo Dice: 0.8074
2024-02-15 12:42:59.835633: 
2024-02-15 12:42:59.835977: Epoch 3
2024-02-15 12:42:59.836171: Current learning rate: 0.00973
2024-02-15 12:44:55.805098: train_loss -0.7481
2024-02-15 12:44:55.805357: val_loss -0.6965
2024-02-15 12:44:55.805428: Pseudo dice [0.8373]
2024-02-15 12:44:55.805502: Epoch time: 115.97 s
2024-02-15 12:44:55.805565: Yayy! New best EMA pseudo Dice: 0.8104
2024-02-15 12:44:57.698535: 
2024-02-15 12:44:57.698658: Epoch 4
2024-02-15 12:44:57.698761: Current learning rate: 0.00964
2024-02-15 12:46:53.453370: train_loss -0.7629
2024-02-15 12:46:53.453635: val_loss -0.695
2024-02-15 12:46:53.453706: Pseudo dice [0.8445]
2024-02-15 12:46:53.453783: Epoch time: 115.76 s
2024-02-15 12:46:53.453838: Yayy! New best EMA pseudo Dice: 0.8138
2024-02-15 12:46:55.314935: 
2024-02-15 12:46:55.315055: Epoch 5
2024-02-15 12:46:55.315154: Current learning rate: 0.00955
2024-02-15 12:48:50.811673: train_loss -0.763
2024-02-15 12:48:50.811950: val_loss -0.6926
2024-02-15 12:48:50.812027: Pseudo dice [0.8455]
2024-02-15 12:48:50.812108: Epoch time: 115.5 s
2024-02-15 12:48:50.812174: Yayy! New best EMA pseudo Dice: 0.817
2024-02-15 12:48:52.469057: 
2024-02-15 12:48:52.469358: Epoch 6
2024-02-15 12:48:52.469624: Current learning rate: 0.00946
2024-02-15 12:50:48.189021: train_loss -0.7675
2024-02-15 12:50:48.189312: val_loss -0.7193
2024-02-15 12:50:48.189392: Pseudo dice [0.8844]
2024-02-15 12:50:48.189478: Epoch time: 115.72 s
2024-02-15 12:50:48.189549: Yayy! New best EMA pseudo Dice: 0.8237
2024-02-15 12:50:50.038659: 
2024-02-15 12:50:50.039005: Epoch 7
2024-02-15 12:50:50.039248: Current learning rate: 0.00937
2024-02-15 12:52:45.842164: train_loss -0.7584
2024-02-15 12:52:45.842432: val_loss -0.7198
2024-02-15 12:52:45.842499: Pseudo dice [0.8853]
2024-02-15 12:52:45.842599: Epoch time: 115.8 s
2024-02-15 12:52:45.842671: Yayy! New best EMA pseudo Dice: 0.8299
2024-02-15 12:52:47.551564: 
2024-02-15 12:52:47.551687: Epoch 8
2024-02-15 12:52:47.551788: Current learning rate: 0.00928
2024-02-15 12:54:43.474857: train_loss -0.774
2024-02-15 12:54:43.475128: val_loss -0.7579
2024-02-15 12:54:43.475199: Pseudo dice [0.8803]
2024-02-15 12:54:43.475275: Epoch time: 115.92 s
2024-02-15 12:54:43.475330: Yayy! New best EMA pseudo Dice: 0.8349
2024-02-15 12:54:45.353285: 
2024-02-15 12:54:45.353410: Epoch 9
2024-02-15 12:54:45.353522: Current learning rate: 0.00919
2024-02-15 12:56:41.277610: train_loss -0.7702
2024-02-15 12:56:41.277862: val_loss -0.7537
2024-02-15 12:56:41.277932: Pseudo dice [0.8756]
2024-02-15 12:56:41.278005: Epoch time: 115.93 s
2024-02-15 12:56:41.278058: Yayy! New best EMA pseudo Dice: 0.839
2024-02-15 12:56:42.894525: 
2024-02-15 12:56:42.894642: Epoch 10
2024-02-15 12:56:42.894742: Current learning rate: 0.0091
2024-02-15 12:58:38.606794: train_loss -0.7599
2024-02-15 12:58:38.607063: val_loss -0.6657
2024-02-15 12:58:38.607131: Pseudo dice [0.8709]
2024-02-15 12:58:38.607204: Epoch time: 115.71 s
2024-02-15 12:58:38.607259: Yayy! New best EMA pseudo Dice: 0.8422
2024-02-15 12:58:40.208350: 
2024-02-15 12:58:40.208468: Epoch 11
2024-02-15 12:58:40.208565: Current learning rate: 0.009
2024-02-15 13:00:36.096238: train_loss -0.7677
2024-02-15 13:00:36.096541: val_loss -0.7333
2024-02-15 13:00:36.096616: Pseudo dice [0.866]
2024-02-15 13:00:36.096704: Epoch time: 115.89 s
2024-02-15 13:00:36.096760: Yayy! New best EMA pseudo Dice: 0.8445
2024-02-15 13:00:37.702573: 
2024-02-15 13:00:37.702694: Epoch 12
2024-02-15 13:00:37.702803: Current learning rate: 0.00891
2024-02-15 13:02:33.488896: train_loss -0.7712
2024-02-15 13:02:33.489178: val_loss -0.7253
2024-02-15 13:02:33.489246: Pseudo dice [0.8736]
2024-02-15 13:02:33.489321: Epoch time: 115.79 s
2024-02-15 13:02:33.489448: Yayy! New best EMA pseudo Dice: 0.8474
2024-02-15 13:02:35.156527: 
2024-02-15 13:02:35.156644: Epoch 13
2024-02-15 13:02:35.156742: Current learning rate: 0.00882
2024-02-15 13:04:30.996576: train_loss -0.7818
2024-02-15 13:04:30.996853: val_loss -0.7323
2024-02-15 13:04:30.996922: Pseudo dice [0.8807]
2024-02-15 13:04:30.997004: Epoch time: 115.84 s
2024-02-15 13:04:30.997072: Yayy! New best EMA pseudo Dice: 0.8508
2024-02-15 13:04:32.836380: 
2024-02-15 13:04:32.836507: Epoch 14
2024-02-15 13:04:32.836603: Current learning rate: 0.00873
2024-02-15 13:06:28.657933: train_loss -0.7853
2024-02-15 13:06:28.658210: val_loss -0.7471
2024-02-15 13:06:28.658278: Pseudo dice [0.8895]
2024-02-15 13:06:28.658353: Epoch time: 115.82 s
2024-02-15 13:06:28.658415: Yayy! New best EMA pseudo Dice: 0.8547
2024-02-15 13:06:30.350119: 
2024-02-15 13:06:30.350243: Epoch 15
2024-02-15 13:06:30.350341: Current learning rate: 0.00864
2024-02-15 13:08:26.236806: train_loss -0.776
2024-02-15 13:08:26.237085: val_loss -0.7268
2024-02-15 13:08:26.237155: Pseudo dice [0.873]
2024-02-15 13:08:26.237232: Epoch time: 115.89 s
2024-02-15 13:08:26.237300: Yayy! New best EMA pseudo Dice: 0.8565
2024-02-15 13:08:28.052362: 
2024-02-15 13:08:28.052483: Epoch 16
2024-02-15 13:08:28.052584: Current learning rate: 0.00855
2024-02-15 13:10:23.835651: train_loss -0.7764
2024-02-15 13:10:23.835916: val_loss -0.7184
2024-02-15 13:10:23.836005: Pseudo dice [0.8828]
2024-02-15 13:10:23.836080: Epoch time: 115.78 s
2024-02-15 13:10:23.836143: Yayy! New best EMA pseudo Dice: 0.8591
2024-02-15 13:10:25.527350: 
2024-02-15 13:10:25.527470: Epoch 17
2024-02-15 13:10:25.527565: Current learning rate: 0.00846
2024-02-15 13:12:21.495315: train_loss -0.7776
2024-02-15 13:12:21.495577: val_loss -0.7291
2024-02-15 13:12:21.495643: Pseudo dice [0.8701]
2024-02-15 13:12:21.495717: Epoch time: 115.97 s
2024-02-15 13:12:21.495771: Yayy! New best EMA pseudo Dice: 0.8602
2024-02-15 13:12:23.213200: 
2024-02-15 13:12:23.213315: Epoch 18
2024-02-15 13:12:23.213414: Current learning rate: 0.00836
2024-02-15 13:14:19.305882: train_loss -0.7882
2024-02-15 13:14:19.306157: val_loss -0.7707
2024-02-15 13:14:19.306223: Pseudo dice [0.8941]
2024-02-15 13:14:19.306298: Epoch time: 116.09 s
2024-02-15 13:14:19.306355: Yayy! New best EMA pseudo Dice: 0.8636
2024-02-15 13:14:21.067141: 
2024-02-15 13:14:21.067261: Epoch 19
2024-02-15 13:14:21.067360: Current learning rate: 0.00827
2024-02-15 13:16:17.044862: train_loss -0.7944
2024-02-15 13:16:17.045147: val_loss -0.7458
2024-02-15 13:16:17.045213: Pseudo dice [0.9074]
2024-02-15 13:16:17.045284: Epoch time: 115.98 s
2024-02-15 13:16:17.045336: Yayy! New best EMA pseudo Dice: 0.868
2024-02-15 13:16:18.890202: 
2024-02-15 13:16:18.890331: Epoch 20
2024-02-15 13:16:18.890442: Current learning rate: 0.00818
2024-02-15 13:18:14.682167: train_loss -0.7933
2024-02-15 13:18:14.682428: val_loss -0.7538
2024-02-15 13:18:14.682493: Pseudo dice [0.892]
2024-02-15 13:18:14.682566: Epoch time: 115.79 s
2024-02-15 13:18:14.682630: Yayy! New best EMA pseudo Dice: 0.8704
2024-02-15 13:18:16.490838: 
2024-02-15 13:18:16.490966: Epoch 21
2024-02-15 13:18:16.491063: Current learning rate: 0.00809
2024-02-15 13:20:12.464396: train_loss -0.7982
2024-02-15 13:20:12.464674: val_loss -0.7022
2024-02-15 13:20:12.464747: Pseudo dice [0.883]
2024-02-15 13:20:12.464825: Epoch time: 115.97 s
2024-02-15 13:20:12.465208: Yayy! New best EMA pseudo Dice: 0.8717
2024-02-15 13:20:14.204659: 
2024-02-15 13:20:14.204940: Epoch 22
2024-02-15 13:20:14.205064: Current learning rate: 0.008
2024-02-15 13:22:10.324037: train_loss -0.8009
2024-02-15 13:22:10.324304: val_loss -0.7513
2024-02-15 13:22:10.324384: Pseudo dice [0.9105]
2024-02-15 13:22:10.324478: Epoch time: 116.12 s
2024-02-15 13:22:10.324544: Yayy! New best EMA pseudo Dice: 0.8755
2024-02-15 13:22:12.087558: 
2024-02-15 13:22:12.087679: Epoch 23
2024-02-15 13:22:12.087790: Current learning rate: 0.0079
2024-02-15 13:24:07.897843: train_loss -0.7852
2024-02-15 13:24:07.898134: val_loss -0.7549
2024-02-15 13:24:07.898205: Pseudo dice [0.8947]
2024-02-15 13:24:07.898284: Epoch time: 115.81 s
2024-02-15 13:24:07.898365: Yayy! New best EMA pseudo Dice: 0.8775
2024-02-15 13:24:09.588125: 
2024-02-15 13:24:09.588238: Epoch 24
2024-02-15 13:24:09.588339: Current learning rate: 0.00781
2024-02-15 13:26:05.571029: train_loss -0.7953
2024-02-15 13:26:05.571327: val_loss -0.7491
2024-02-15 13:26:05.571396: Pseudo dice [0.8899]
2024-02-15 13:26:05.571483: Epoch time: 115.98 s
2024-02-15 13:26:05.571534: Yayy! New best EMA pseudo Dice: 0.8787
2024-02-15 13:26:07.599183: 
2024-02-15 13:26:07.599298: Epoch 25
2024-02-15 13:26:07.599397: Current learning rate: 0.00772
2024-02-15 13:28:03.670354: train_loss -0.8074
2024-02-15 13:28:03.670617: val_loss -0.7612
2024-02-15 13:28:03.670688: Pseudo dice [0.8953]
2024-02-15 13:28:03.670764: Epoch time: 116.07 s
2024-02-15 13:28:03.670821: Yayy! New best EMA pseudo Dice: 0.8804
2024-02-15 13:28:05.277832: 
2024-02-15 13:28:05.277971: Epoch 26
2024-02-15 13:28:05.278069: Current learning rate: 0.00763
2024-02-15 13:30:01.137223: train_loss -0.8071
2024-02-15 13:30:01.137494: val_loss -0.768
2024-02-15 13:30:01.137563: Pseudo dice [0.8978]
2024-02-15 13:30:01.137637: Epoch time: 115.86 s
2024-02-15 13:30:01.137703: Yayy! New best EMA pseudo Dice: 0.8821
2024-02-15 13:30:02.809411: 
2024-02-15 13:30:02.809528: Epoch 27
2024-02-15 13:30:02.809624: Current learning rate: 0.00753
2024-02-15 13:31:58.650906: train_loss -0.8052
2024-02-15 13:31:58.651176: val_loss -0.7777
2024-02-15 13:31:58.651244: Pseudo dice [0.9029]
2024-02-15 13:31:58.651318: Epoch time: 115.84 s
2024-02-15 13:31:58.651372: Yayy! New best EMA pseudo Dice: 0.8842
2024-02-15 13:32:00.305015: 
2024-02-15 13:32:00.305241: Epoch 28
2024-02-15 13:32:00.305415: Current learning rate: 0.00744
2024-02-15 13:33:56.195834: train_loss -0.7932
2024-02-15 13:33:56.196252: val_loss -0.7512
2024-02-15 13:33:56.196426: Pseudo dice [0.8949]
2024-02-15 13:33:56.196572: Epoch time: 115.89 s
2024-02-15 13:33:56.196627: Yayy! New best EMA pseudo Dice: 0.8853
2024-02-15 13:33:57.839676: 
2024-02-15 13:33:57.839798: Epoch 29
2024-02-15 13:33:57.839899: Current learning rate: 0.00735
2024-02-15 13:35:53.651850: train_loss -0.8083
2024-02-15 13:35:53.652144: val_loss -0.7513
2024-02-15 13:35:53.652218: Pseudo dice [0.8922]
2024-02-15 13:35:53.652298: Epoch time: 115.81 s
2024-02-15 13:35:53.652362: Yayy! New best EMA pseudo Dice: 0.886
2024-02-15 13:35:55.531755: 
2024-02-15 13:35:55.532186: Epoch 30
2024-02-15 13:35:55.532348: Current learning rate: 0.00725
2024-02-15 13:37:51.603999: train_loss -0.8074
2024-02-15 13:37:51.604280: val_loss -0.7354
2024-02-15 13:37:51.604346: Pseudo dice [0.9009]
2024-02-15 13:37:51.604421: Epoch time: 116.07 s
2024-02-15 13:37:51.604483: Yayy! New best EMA pseudo Dice: 0.8874
2024-02-15 13:37:53.259614: 
2024-02-15 13:37:53.259731: Epoch 31
2024-02-15 13:37:53.259828: Current learning rate: 0.00716
2024-02-15 13:39:49.210167: train_loss -0.8053
2024-02-15 13:39:49.210474: val_loss -0.7633
2024-02-15 13:39:49.210555: Pseudo dice [0.8987]
2024-02-15 13:39:49.210648: Epoch time: 115.95 s
2024-02-15 13:39:49.210702: Yayy! New best EMA pseudo Dice: 0.8886
2024-02-15 13:39:50.897398: 
2024-02-15 13:39:50.897559: Epoch 32
2024-02-15 13:39:50.897657: Current learning rate: 0.00707
2024-02-15 13:41:46.807987: train_loss -0.7855
2024-02-15 13:41:46.808285: val_loss -0.7654
2024-02-15 13:41:46.808358: Pseudo dice [0.8836]
2024-02-15 13:41:46.808441: Epoch time: 115.91 s
2024-02-15 13:41:48.086471: 
2024-02-15 13:41:48.086604: Epoch 33
2024-02-15 13:41:48.086713: Current learning rate: 0.00697
2024-02-15 13:43:44.126495: train_loss -0.8023
2024-02-15 13:43:44.126767: val_loss -0.7372
2024-02-15 13:43:44.126837: Pseudo dice [0.8917]
2024-02-15 13:43:44.126915: Epoch time: 116.04 s
2024-02-15 13:43:45.354741: 
2024-02-15 13:43:45.354860: Epoch 34
2024-02-15 13:43:45.354976: Current learning rate: 0.00688
2024-02-15 13:45:41.399853: train_loss -0.8165
2024-02-15 13:45:41.400137: val_loss -0.7309
2024-02-15 13:45:41.400208: Pseudo dice [0.8955]
2024-02-15 13:45:41.400295: Epoch time: 116.05 s
2024-02-15 13:45:41.400360: Yayy! New best EMA pseudo Dice: 0.8892
2024-02-15 13:45:43.126718: 
2024-02-15 13:45:43.126835: Epoch 35
2024-02-15 13:45:43.126945: Current learning rate: 0.00679
2024-02-15 13:47:39.147731: train_loss -0.8072
2024-02-15 13:47:39.148009: val_loss -0.7613
2024-02-15 13:47:39.148081: Pseudo dice [0.908]
2024-02-15 13:47:39.148160: Epoch time: 116.02 s
2024-02-15 13:47:39.148215: Yayy! New best EMA pseudo Dice: 0.891
2024-02-15 13:47:42.602291: 
2024-02-15 13:47:42.602637: Epoch 36
2024-02-15 13:47:42.602767: Current learning rate: 0.00669
2024-02-15 13:49:38.615792: train_loss -0.8303
2024-02-15 13:49:38.616066: val_loss -0.7538
2024-02-15 13:49:38.616144: Pseudo dice [0.8997]
2024-02-15 13:49:38.616216: Epoch time: 116.01 s
2024-02-15 13:49:38.616268: Yayy! New best EMA pseudo Dice: 0.8919
2024-02-15 13:49:40.274664: 
2024-02-15 13:49:40.274996: Epoch 37
2024-02-15 13:49:40.275102: Current learning rate: 0.0066
2024-02-15 13:51:36.086125: train_loss -0.8273
2024-02-15 13:51:36.086404: val_loss -0.7286
2024-02-15 13:51:36.086480: Pseudo dice [0.8851]
2024-02-15 13:51:36.086559: Epoch time: 115.81 s
2024-02-15 13:51:37.364651: 
2024-02-15 13:51:37.364778: Epoch 38
2024-02-15 13:51:37.364880: Current learning rate: 0.0065
2024-02-15 13:53:33.402271: train_loss -0.8223
2024-02-15 13:53:33.402544: val_loss -0.7795
2024-02-15 13:53:33.402616: Pseudo dice [0.8995]
2024-02-15 13:53:33.402694: Epoch time: 116.04 s
2024-02-15 13:53:33.402762: Yayy! New best EMA pseudo Dice: 0.8921
2024-02-15 13:53:35.089403: 
2024-02-15 13:53:35.089521: Epoch 39
2024-02-15 13:53:35.089620: Current learning rate: 0.00641
2024-02-15 13:55:31.098774: train_loss -0.7982
2024-02-15 13:55:31.099051: val_loss -0.7695
2024-02-15 13:55:31.099117: Pseudo dice [0.899]
2024-02-15 13:55:31.099192: Epoch time: 116.01 s
2024-02-15 13:55:31.099261: Yayy! New best EMA pseudo Dice: 0.8928
2024-02-15 13:55:32.794088: 
2024-02-15 13:55:32.794202: Epoch 40
2024-02-15 13:55:32.794296: Current learning rate: 0.00631
2024-02-15 13:57:28.826991: train_loss -0.8149
2024-02-15 13:57:28.827257: val_loss -0.7533
2024-02-15 13:57:28.827321: Pseudo dice [0.8925]
2024-02-15 13:57:28.827391: Epoch time: 116.03 s
2024-02-15 13:57:30.238254: 
2024-02-15 13:57:30.238370: Epoch 41
2024-02-15 13:57:30.238478: Current learning rate: 0.00622
2024-02-15 13:59:26.104655: train_loss -0.8081
2024-02-15 13:59:26.104930: val_loss -0.7754
2024-02-15 13:59:26.104999: Pseudo dice [0.9043]
2024-02-15 13:59:26.105073: Epoch time: 115.87 s
2024-02-15 13:59:26.105145: Yayy! New best EMA pseudo Dice: 0.8939
2024-02-15 13:59:27.729291: 
2024-02-15 13:59:27.729404: Epoch 42
2024-02-15 13:59:27.729498: Current learning rate: 0.00612
2024-02-15 14:01:23.719978: train_loss -0.8255
2024-02-15 14:01:23.720262: val_loss -0.7639
2024-02-15 14:01:23.720327: Pseudo dice [0.8914]
2024-02-15 14:01:23.720398: Epoch time: 115.99 s
2024-02-15 14:01:24.892180: 
2024-02-15 14:01:24.892319: Epoch 43
2024-02-15 14:01:24.892420: Current learning rate: 0.00603
2024-02-15 14:03:20.871753: train_loss -0.8153
2024-02-15 14:03:20.872187: val_loss -0.7942
2024-02-15 14:03:20.872356: Pseudo dice [0.8928]
2024-02-15 14:03:20.872468: Epoch time: 115.98 s
2024-02-15 14:03:22.055863: 
2024-02-15 14:03:22.055993: Epoch 44
2024-02-15 14:03:22.056103: Current learning rate: 0.00593
2024-02-15 14:05:17.994499: train_loss -0.8188
2024-02-15 14:05:17.994851: val_loss -0.7672
2024-02-15 14:05:17.994985: Pseudo dice [0.9003]
2024-02-15 14:05:17.995101: Epoch time: 115.94 s
2024-02-15 14:05:17.995192: Yayy! New best EMA pseudo Dice: 0.8942
2024-02-15 14:05:19.657424: 
2024-02-15 14:05:19.657540: Epoch 45
2024-02-15 14:05:19.657640: Current learning rate: 0.00584
2024-02-15 14:07:15.814242: train_loss -0.8049
2024-02-15 14:07:15.814515: val_loss -0.7212
2024-02-15 14:07:15.814590: Pseudo dice [0.8928]
2024-02-15 14:07:15.814664: Epoch time: 116.16 s
2024-02-15 14:07:17.082000: 
2024-02-15 14:07:17.082123: Epoch 46
2024-02-15 14:07:17.082226: Current learning rate: 0.00574
2024-02-15 14:09:13.003731: train_loss -0.8068
2024-02-15 14:09:13.004002: val_loss -0.7211
2024-02-15 14:09:13.004073: Pseudo dice [0.8878]
2024-02-15 14:09:13.004148: Epoch time: 115.92 s
2024-02-15 14:09:14.324146: 
2024-02-15 14:09:14.324273: Epoch 47
2024-02-15 14:09:14.324389: Current learning rate: 0.00565
2024-02-15 14:11:10.339787: train_loss -0.8079
2024-02-15 14:11:10.340081: val_loss -0.7473
2024-02-15 14:11:10.340153: Pseudo dice [0.899]
2024-02-15 14:11:10.340228: Epoch time: 116.02 s
2024-02-15 14:11:11.488613: 
2024-02-15 14:11:11.488736: Epoch 48
2024-02-15 14:11:11.488832: Current learning rate: 0.00555
2024-02-15 14:13:07.470976: train_loss -0.8315
2024-02-15 14:13:07.471253: val_loss -0.7445
2024-02-15 14:13:07.471323: Pseudo dice [0.8968]
2024-02-15 14:13:07.471396: Epoch time: 115.98 s
2024-02-15 14:13:07.471462: Yayy! New best EMA pseudo Dice: 0.8943
2024-02-15 14:13:09.225365: 
2024-02-15 14:13:09.225668: Epoch 49
2024-02-15 14:13:09.225901: Current learning rate: 0.00546
2024-02-15 14:15:05.310529: train_loss -0.8303
2024-02-15 14:15:05.310881: val_loss -0.7283
2024-02-15 14:15:05.311015: Pseudo dice [0.8912]
2024-02-15 14:15:05.311135: Epoch time: 116.09 s
2024-02-15 14:15:06.839236: 
2024-02-15 14:15:06.839356: Epoch 50
2024-02-15 14:15:06.839452: Current learning rate: 0.00536
2024-02-15 14:17:02.915766: train_loss -0.8218
2024-02-15 14:17:02.916062: val_loss -0.7338
2024-02-15 14:17:02.916136: Pseudo dice [0.8954]
2024-02-15 14:17:02.916220: Epoch time: 116.08 s
2024-02-15 14:17:04.241593: 
2024-02-15 14:17:04.241709: Epoch 51
2024-02-15 14:17:04.241805: Current learning rate: 0.00526
2024-02-15 14:19:00.432193: train_loss -0.807
2024-02-15 14:19:00.432468: val_loss -0.7494
2024-02-15 14:19:00.432542: Pseudo dice [0.9024]
2024-02-15 14:19:00.432618: Epoch time: 116.19 s
2024-02-15 14:19:00.432686: Yayy! New best EMA pseudo Dice: 0.895
2024-02-15 14:19:02.180052: 
2024-02-15 14:19:02.180166: Epoch 52
2024-02-15 14:19:02.180264: Current learning rate: 0.00517
2024-02-15 14:20:58.317749: train_loss -0.8291
2024-02-15 14:20:58.318174: val_loss -0.7647
2024-02-15 14:20:58.318387: Pseudo dice [0.9036]
2024-02-15 14:20:58.318464: Epoch time: 116.14 s
2024-02-15 14:20:58.318523: Yayy! New best EMA pseudo Dice: 0.8958
2024-02-15 14:21:00.131696: 
2024-02-15 14:21:00.131999: Epoch 53
2024-02-15 14:21:00.132178: Current learning rate: 0.00507
2024-02-15 14:22:55.992606: train_loss -0.8269
2024-02-15 14:22:55.992874: val_loss -0.8015
2024-02-15 14:22:55.992947: Pseudo dice [0.9035]
2024-02-15 14:22:55.993021: Epoch time: 115.86 s
2024-02-15 14:22:55.993084: Yayy! New best EMA pseudo Dice: 0.8966
2024-02-15 14:22:57.641037: 
2024-02-15 14:22:57.641247: Epoch 54
2024-02-15 14:22:57.641416: Current learning rate: 0.00497
2024-02-15 14:24:53.372755: train_loss -0.8217
2024-02-15 14:24:53.373023: val_loss -0.7521
2024-02-15 14:24:53.373090: Pseudo dice [0.8902]
2024-02-15 14:24:53.373171: Epoch time: 115.73 s
2024-02-15 14:24:54.597594: 
2024-02-15 14:24:54.597723: Epoch 55
2024-02-15 14:24:54.597821: Current learning rate: 0.00487
2024-02-15 14:26:50.669627: train_loss -0.8243
2024-02-15 14:26:50.669877: val_loss -0.7889
2024-02-15 14:26:50.669967: Pseudo dice [0.9058]
2024-02-15 14:26:50.670044: Epoch time: 116.07 s
2024-02-15 14:26:50.670099: Yayy! New best EMA pseudo Dice: 0.8969
2024-02-15 14:26:52.350686: 
2024-02-15 14:26:52.350813: Epoch 56
2024-02-15 14:26:52.350914: Current learning rate: 0.00478
2024-02-15 14:28:48.615711: train_loss -0.8218
2024-02-15 14:28:48.616003: val_loss -0.762
2024-02-15 14:28:48.616078: Pseudo dice [0.902]
2024-02-15 14:28:48.616167: Epoch time: 116.27 s
2024-02-15 14:28:48.616234: Yayy! New best EMA pseudo Dice: 0.8974
2024-02-15 14:28:50.393973: 
2024-02-15 14:28:50.394092: Epoch 57
2024-02-15 14:28:50.394197: Current learning rate: 0.00468
2024-02-15 14:30:46.376170: train_loss -0.838
2024-02-15 14:30:46.376462: val_loss -0.7565
2024-02-15 14:30:46.376532: Pseudo dice [0.8962]
2024-02-15 14:30:46.376607: Epoch time: 115.98 s
2024-02-15 14:30:47.728775: 
2024-02-15 14:30:47.729100: Epoch 58
2024-02-15 14:30:47.729281: Current learning rate: 0.00458
2024-02-15 14:32:43.428595: train_loss -0.8339
2024-02-15 14:32:43.428903: val_loss -0.7541
2024-02-15 14:32:43.428988: Pseudo dice [0.9]
2024-02-15 14:32:43.429100: Epoch time: 115.7 s
2024-02-15 14:32:43.429155: Yayy! New best EMA pseudo Dice: 0.8976
2024-02-15 14:32:45.100009: 
2024-02-15 14:32:45.100484: Epoch 59
2024-02-15 14:32:45.100652: Current learning rate: 0.00448
2024-02-15 14:34:40.865997: train_loss -0.8125
2024-02-15 14:34:40.866280: val_loss -0.754
2024-02-15 14:34:40.866350: Pseudo dice [0.8914]
2024-02-15 14:34:40.866427: Epoch time: 115.77 s
2024-02-15 14:34:42.199736: 
2024-02-15 14:34:42.199867: Epoch 60
2024-02-15 14:34:42.199988: Current learning rate: 0.00438
2024-02-15 14:36:38.218563: train_loss -0.836
2024-02-15 14:36:38.218830: val_loss -0.7343
2024-02-15 14:36:38.218897: Pseudo dice [0.9032]
2024-02-15 14:36:38.218979: Epoch time: 116.02 s
2024-02-15 14:36:38.219044: Yayy! New best EMA pseudo Dice: 0.8976
2024-02-15 14:36:39.977973: 
2024-02-15 14:36:39.978094: Epoch 61
2024-02-15 14:36:39.978199: Current learning rate: 0.00429
2024-02-15 14:38:35.868666: train_loss -0.8253
2024-02-15 14:38:35.868948: val_loss -0.7718
2024-02-15 14:38:35.869017: Pseudo dice [0.9012]
2024-02-15 14:38:35.869095: Epoch time: 115.89 s
2024-02-15 14:38:35.869158: Yayy! New best EMA pseudo Dice: 0.898
2024-02-15 14:38:37.559615: 
2024-02-15 14:38:37.559917: Epoch 62
2024-02-15 14:38:37.560149: Current learning rate: 0.00419
2024-02-15 14:40:33.275339: train_loss -0.8401
2024-02-15 14:40:33.275746: val_loss -0.7578
2024-02-15 14:40:33.275921: Pseudo dice [0.8958]
2024-02-15 14:40:33.276004: Epoch time: 115.72 s
2024-02-15 14:40:34.476912: 
2024-02-15 14:40:34.477028: Epoch 63
2024-02-15 14:40:34.477121: Current learning rate: 0.00409
2024-02-15 14:42:30.517981: train_loss -0.8183
2024-02-15 14:42:30.518257: val_loss -0.7218
2024-02-15 14:42:30.518326: Pseudo dice [0.8963]
2024-02-15 14:42:30.518402: Epoch time: 116.04 s
2024-02-15 14:42:31.890307: 
2024-02-15 14:42:31.890426: Epoch 64
2024-02-15 14:42:31.890524: Current learning rate: 0.00399
2024-02-15 14:44:27.862735: train_loss -0.823
2024-02-15 14:44:27.863001: val_loss -0.7633
2024-02-15 14:44:27.863067: Pseudo dice [0.9041]
2024-02-15 14:44:27.863150: Epoch time: 115.97 s
2024-02-15 14:44:27.863200: Yayy! New best EMA pseudo Dice: 0.8982
2024-02-15 14:44:29.553544: 
2024-02-15 14:44:29.553667: Epoch 65
2024-02-15 14:44:29.553761: Current learning rate: 0.00389
2024-02-15 14:46:25.703814: train_loss -0.8173
2024-02-15 14:46:25.704114: val_loss -0.7663
2024-02-15 14:46:25.704177: Pseudo dice [0.9048]
2024-02-15 14:46:25.704249: Epoch time: 116.15 s
2024-02-15 14:46:25.704302: Yayy! New best EMA pseudo Dice: 0.8989
2024-02-15 14:46:27.374500: 
2024-02-15 14:46:27.374702: Epoch 66
2024-02-15 14:46:27.374885: Current learning rate: 0.00379
2024-02-15 14:48:23.419373: train_loss -0.8274
2024-02-15 14:48:23.419665: val_loss -0.7348
2024-02-15 14:48:23.419736: Pseudo dice [0.9016]
2024-02-15 14:48:23.419817: Epoch time: 116.05 s
2024-02-15 14:48:23.419884: Yayy! New best EMA pseudo Dice: 0.8992
2024-02-15 14:48:25.274137: 
2024-02-15 14:48:25.274351: Epoch 67
2024-02-15 14:48:25.274557: Current learning rate: 0.00369
2024-02-15 14:50:21.198388: train_loss -0.8296
2024-02-15 14:50:21.198967: val_loss -0.7472
2024-02-15 14:50:21.199044: Pseudo dice [0.8979]
2024-02-15 14:50:21.199117: Epoch time: 115.93 s
2024-02-15 14:50:22.422401: 
2024-02-15 14:50:22.422521: Epoch 68
2024-02-15 14:50:22.422618: Current learning rate: 0.00359
2024-02-15 14:52:18.412945: train_loss -0.8344
2024-02-15 14:52:18.413214: val_loss -0.7864
2024-02-15 14:52:18.413282: Pseudo dice [0.8886]
2024-02-15 14:52:18.413359: Epoch time: 115.99 s
2024-02-15 14:52:19.630541: 
2024-02-15 14:52:19.630663: Epoch 69
2024-02-15 14:52:19.630759: Current learning rate: 0.00349
2024-02-15 14:54:17.574283: train_loss -0.8334
2024-02-15 14:54:17.574488: val_loss -0.7709
2024-02-15 14:54:17.574556: Pseudo dice [0.897]
2024-02-15 14:54:17.574623: Epoch time: 117.94 s
2024-02-15 14:54:18.769068: 
2024-02-15 14:54:18.769212: Epoch 70
2024-02-15 14:54:18.769322: Current learning rate: 0.00338
2024-02-15 14:56:14.738252: train_loss -0.8459
2024-02-15 14:56:14.738507: val_loss -0.751
2024-02-15 14:56:14.738578: Pseudo dice [0.9051]
2024-02-15 14:56:14.738649: Epoch time: 115.97 s
2024-02-15 14:56:15.947514: 
2024-02-15 14:56:15.947635: Epoch 71
2024-02-15 14:56:15.947738: Current learning rate: 0.00328
2024-02-15 14:58:12.134540: train_loss -0.8399
2024-02-15 14:58:12.134797: val_loss -0.7686
2024-02-15 14:58:12.134861: Pseudo dice [0.9046]
2024-02-15 14:58:12.134942: Epoch time: 116.19 s
2024-02-15 14:58:12.135000: Yayy! New best EMA pseudo Dice: 0.8992
2024-02-15 14:58:13.883820: 
2024-02-15 14:58:13.883954: Epoch 72
2024-02-15 14:58:13.884053: Current learning rate: 0.00318
2024-02-15 15:00:09.722400: train_loss -0.834
2024-02-15 15:00:09.722685: val_loss -0.7728
2024-02-15 15:00:09.722753: Pseudo dice [0.8978]
2024-02-15 15:00:09.722829: Epoch time: 115.84 s
2024-02-15 15:00:10.985209: 
2024-02-15 15:00:10.985335: Epoch 73
2024-02-15 15:00:10.985435: Current learning rate: 0.00308
2024-02-15 15:02:07.057216: train_loss -0.8273
2024-02-15 15:02:07.057497: val_loss -0.7609
2024-02-15 15:02:07.057572: Pseudo dice [0.8955]
2024-02-15 15:02:07.057649: Epoch time: 116.07 s
2024-02-15 15:02:08.276569: 
2024-02-15 15:02:08.276693: Epoch 74
2024-02-15 15:02:08.276798: Current learning rate: 0.00297
2024-02-15 15:04:04.362894: train_loss -0.8386
2024-02-15 15:04:04.363171: val_loss -0.767
2024-02-15 15:04:04.363241: Pseudo dice [0.8952]
2024-02-15 15:04:04.363317: Epoch time: 116.09 s
2024-02-15 15:04:05.767886: 
2024-02-15 15:04:05.768023: Epoch 75
2024-02-15 15:04:05.768140: Current learning rate: 0.00287
2024-02-15 15:06:01.850900: train_loss -0.8296
2024-02-15 15:06:01.851221: val_loss -0.8029
2024-02-15 15:06:01.851290: Pseudo dice [0.9049]
2024-02-15 15:06:01.851374: Epoch time: 116.08 s
2024-02-15 15:06:03.074053: 
2024-02-15 15:06:03.074188: Epoch 76
2024-02-15 15:06:03.074285: Current learning rate: 0.00277
2024-02-15 15:07:59.175443: train_loss -0.8292
2024-02-15 15:07:59.175699: val_loss -0.7601
2024-02-15 15:07:59.175770: Pseudo dice [0.9078]
2024-02-15 15:07:59.175844: Epoch time: 116.1 s
2024-02-15 15:07:59.175909: Yayy! New best EMA pseudo Dice: 0.8999
2024-02-15 15:08:00.912116: 
2024-02-15 15:08:00.912241: Epoch 77
2024-02-15 15:08:00.912340: Current learning rate: 0.00266
2024-02-15 15:09:56.839246: train_loss -0.8383
2024-02-15 15:09:56.839499: val_loss -0.7434
2024-02-15 15:09:56.839563: Pseudo dice [0.8978]
2024-02-15 15:09:56.839634: Epoch time: 115.93 s
2024-02-15 15:09:58.068390: 
2024-02-15 15:09:58.068504: Epoch 78
2024-02-15 15:09:58.068603: Current learning rate: 0.00256
2024-02-15 15:11:54.250718: train_loss -0.8395
2024-02-15 15:11:54.251006: val_loss -0.7559
2024-02-15 15:11:54.251077: Pseudo dice [0.8941]
2024-02-15 15:11:54.251156: Epoch time: 116.18 s
2024-02-15 15:11:55.509972: 
2024-02-15 15:11:55.510085: Epoch 79
2024-02-15 15:11:55.510181: Current learning rate: 0.00245
2024-02-15 15:13:51.652760: train_loss -0.8269
2024-02-15 15:13:51.653040: val_loss -0.7446
2024-02-15 15:13:51.653121: Pseudo dice [0.8897]
2024-02-15 15:13:51.653194: Epoch time: 116.14 s
2024-02-15 15:13:53.073359: 
2024-02-15 15:13:53.073573: Epoch 80
2024-02-15 15:13:53.073750: Current learning rate: 0.00235
2024-02-15 15:15:49.102546: train_loss -0.8287
2024-02-15 15:15:49.102812: val_loss -0.7485
2024-02-15 15:15:49.102881: Pseudo dice [0.8937]
2024-02-15 15:15:49.102962: Epoch time: 116.03 s
2024-02-15 15:15:50.347594: 
2024-02-15 15:15:50.347710: Epoch 81
2024-02-15 15:15:50.347804: Current learning rate: 0.00224
2024-02-15 15:17:46.345483: train_loss -0.8491
2024-02-15 15:17:46.345844: val_loss -0.7979
2024-02-15 15:17:46.346143: Pseudo dice [0.9092]
2024-02-15 15:17:46.346219: Epoch time: 116.0 s
2024-02-15 15:17:47.584508: 
2024-02-15 15:17:47.584631: Epoch 82
2024-02-15 15:17:47.584724: Current learning rate: 0.00214
2024-02-15 15:19:43.738940: train_loss -0.8523
2024-02-15 15:19:43.739218: val_loss -0.753
2024-02-15 15:19:43.739288: Pseudo dice [0.9054]
2024-02-15 15:19:43.739362: Epoch time: 116.16 s
2024-02-15 15:19:44.917043: 
2024-02-15 15:19:44.917154: Epoch 83
2024-02-15 15:19:44.917259: Current learning rate: 0.00203
2024-02-15 15:21:40.945134: train_loss -0.8415
2024-02-15 15:21:40.945416: val_loss -0.7625
2024-02-15 15:21:40.945487: Pseudo dice [0.8955]
2024-02-15 15:21:40.945567: Epoch time: 116.03 s
2024-02-15 15:21:42.141265: 
2024-02-15 15:21:42.141393: Epoch 84
2024-02-15 15:21:42.141495: Current learning rate: 0.00192
2024-02-15 15:23:38.116625: train_loss -0.8359
2024-02-15 15:23:38.116923: val_loss -0.7414
2024-02-15 15:23:38.117012: Pseudo dice [0.8985]
2024-02-15 15:23:38.117116: Epoch time: 115.98 s
2024-02-15 15:23:39.276411: 
2024-02-15 15:23:39.276604: Epoch 85
2024-02-15 15:23:39.276756: Current learning rate: 0.00181
2024-02-15 15:25:35.272345: train_loss -0.8358
2024-02-15 15:25:35.272646: val_loss -0.7562
2024-02-15 15:25:35.272729: Pseudo dice [0.8971]
2024-02-15 15:25:35.272822: Epoch time: 116.0 s
2024-02-15 15:25:36.615606: 
2024-02-15 15:25:36.615843: Epoch 86
2024-02-15 15:25:36.616056: Current learning rate: 0.0017
2024-02-15 15:27:32.847833: train_loss -0.8556
2024-02-15 15:27:32.848105: val_loss -0.7923
2024-02-15 15:27:32.848175: Pseudo dice [0.9059]
2024-02-15 15:27:32.848249: Epoch time: 116.23 s
2024-02-15 15:27:34.146335: 
2024-02-15 15:27:34.146462: Epoch 87
2024-02-15 15:27:34.146557: Current learning rate: 0.00159
2024-02-15 15:29:30.105410: train_loss -0.8367
2024-02-15 15:29:30.105685: val_loss -0.7508
2024-02-15 15:29:30.105755: Pseudo dice [0.8909]
2024-02-15 15:29:30.105831: Epoch time: 115.96 s
2024-02-15 15:29:31.260508: 
2024-02-15 15:29:31.260630: Epoch 88
2024-02-15 15:29:31.260740: Current learning rate: 0.00148
2024-02-15 15:31:27.280564: train_loss -0.8582
2024-02-15 15:31:27.280843: val_loss -0.788
2024-02-15 15:31:27.280913: Pseudo dice [0.9018]
2024-02-15 15:31:27.280994: Epoch time: 116.02 s
2024-02-15 15:31:28.435486: 
2024-02-15 15:31:28.435755: Epoch 89
2024-02-15 15:31:28.435937: Current learning rate: 0.00137
2024-02-15 15:33:24.275855: train_loss -0.8504
2024-02-15 15:33:24.276155: val_loss -0.7818
2024-02-15 15:33:24.276232: Pseudo dice [0.8987]
2024-02-15 15:33:24.276321: Epoch time: 115.84 s
2024-02-15 15:33:25.568435: 
2024-02-15 15:33:25.568552: Epoch 90
2024-02-15 15:33:25.568648: Current learning rate: 0.00126
2024-02-15 15:35:21.719500: train_loss -0.8642
2024-02-15 15:35:21.719779: val_loss -0.7584
2024-02-15 15:35:21.719861: Pseudo dice [0.899]
2024-02-15 15:35:21.719957: Epoch time: 116.15 s
2024-02-15 15:35:23.008383: 
2024-02-15 15:35:23.008641: Epoch 91
2024-02-15 15:35:23.008877: Current learning rate: 0.00115
2024-02-15 15:37:19.145143: train_loss -0.8446
2024-02-15 15:37:19.145410: val_loss -0.773
2024-02-15 15:37:19.145478: Pseudo dice [0.9099]
2024-02-15 15:37:19.145550: Epoch time: 116.14 s
2024-02-15 15:37:19.145609: Yayy! New best EMA pseudo Dice: 0.9001
2024-02-15 15:37:21.002660: 
2024-02-15 15:37:21.002790: Epoch 92
2024-02-15 15:37:21.002891: Current learning rate: 0.00103
2024-02-15 15:39:16.961688: train_loss -0.841
2024-02-15 15:39:16.961921: val_loss -0.7506
2024-02-15 15:39:16.962004: Pseudo dice [0.8966]
2024-02-15 15:39:16.962075: Epoch time: 115.96 s
2024-02-15 15:39:18.114097: 
2024-02-15 15:39:18.114335: Epoch 93
2024-02-15 15:39:18.114558: Current learning rate: 0.00091
2024-02-15 15:41:14.076587: train_loss -0.8455
2024-02-15 15:41:14.076878: val_loss -0.7867
2024-02-15 15:41:14.076955: Pseudo dice [0.9021]
2024-02-15 15:41:14.077047: Epoch time: 115.96 s
2024-02-15 15:41:15.235344: 
2024-02-15 15:41:15.235482: Epoch 94
2024-02-15 15:41:15.235588: Current learning rate: 0.00079
2024-02-15 15:43:11.179560: train_loss -0.8474
2024-02-15 15:43:11.179831: val_loss -0.7586
2024-02-15 15:43:11.179898: Pseudo dice [0.9055]
2024-02-15 15:43:11.179991: Epoch time: 115.95 s
2024-02-15 15:43:11.180059: Yayy! New best EMA pseudo Dice: 0.9005
2024-02-15 15:43:12.899766: 
2024-02-15 15:43:12.899889: Epoch 95
2024-02-15 15:43:12.899997: Current learning rate: 0.00067
2024-02-15 15:45:08.715065: train_loss -0.8568
2024-02-15 15:45:08.715369: val_loss -0.7575
2024-02-15 15:45:08.715451: Pseudo dice [0.9011]
2024-02-15 15:45:08.715547: Epoch time: 115.82 s
2024-02-15 15:45:08.715606: Yayy! New best EMA pseudo Dice: 0.9006
2024-02-15 15:45:10.453366: 
2024-02-15 15:45:10.453680: Epoch 96
2024-02-15 15:45:10.453979: Current learning rate: 0.00055
2024-02-15 15:47:06.435526: train_loss -0.8491
2024-02-15 15:47:06.435790: val_loss -0.7627
2024-02-15 15:47:06.435858: Pseudo dice [0.9011]
2024-02-15 15:47:06.435943: Epoch time: 115.98 s
2024-02-15 15:47:06.435999: Yayy! New best EMA pseudo Dice: 0.9006
2024-02-15 15:47:08.275251: 
2024-02-15 15:47:08.275378: Epoch 97
2024-02-15 15:47:08.275504: Current learning rate: 0.00043
2024-02-15 15:49:04.334167: train_loss -0.8493
2024-02-15 15:49:04.334439: val_loss -0.7542
2024-02-15 15:49:04.334512: Pseudo dice [0.894]
2024-02-15 15:49:04.334586: Epoch time: 116.06 s
2024-02-15 15:49:05.585411: 
2024-02-15 15:49:05.585535: Epoch 98
2024-02-15 15:49:05.585634: Current learning rate: 0.0003
2024-02-15 15:51:01.427385: train_loss -0.8422
2024-02-15 15:51:01.427667: val_loss -0.7677
2024-02-15 15:51:01.427739: Pseudo dice [0.9052]
2024-02-15 15:51:01.427819: Epoch time: 115.84 s
2024-02-15 15:51:02.639284: 
2024-02-15 15:51:02.639436: Epoch 99
2024-02-15 15:51:02.639549: Current learning rate: 0.00016
2024-02-15 15:52:58.985008: train_loss -0.8607
2024-02-15 15:52:58.985386: val_loss -0.7665
2024-02-15 15:52:58.985579: Pseudo dice [0.8989]
2024-02-15 15:52:58.985760: Epoch time: 116.35 s
2024-02-15 15:53:00.742074: Training done.
2024-02-15 15:53:00.792374: Using splits from existing split file: /nvmescratch/ceib/Prostate/nnUnet/nnUNet_preprocessed/Dataset014_ProstateOwn/splits_final.json
2024-02-15 15:53:00.792833: The split file contains 5 splits.
2024-02-15 15:53:00.792903: Desired fold for training: 4
2024-02-15 15:53:00.792974: This split has 132 training and 32 validation cases.
2024-02-15 15:53:00.793361: predicting ProstateOwn_1
2024-02-15 15:53:00.794626: ProstateOwn_1, shape torch.Size([1, 42, 308, 308]), rank 0
2024-02-15 15:53:06.417029: predicting ProstateOwn_104
2024-02-15 15:53:06.420149: ProstateOwn_104, shape torch.Size([1, 63, 411, 411]), rank 0
2024-02-15 15:53:20.416357: predicting ProstateOwn_108
2024-02-15 15:53:20.419461: ProstateOwn_108, shape torch.Size([1, 30, 308, 308]), rank 0
2024-02-15 15:53:22.918849: predicting ProstateOwn_119
2024-02-15 15:53:22.920559: ProstateOwn_119, shape torch.Size([1, 34, 307, 307]), rank 0
2024-02-15 15:53:25.415149: predicting ProstateOwn_124
2024-02-15 15:53:25.417070: ProstateOwn_124, shape torch.Size([1, 37, 308, 308]), rank 0
2024-02-15 15:53:29.150684: predicting ProstateOwn_126
2024-02-15 15:53:29.152236: ProstateOwn_126, shape torch.Size([1, 68, 475, 475]), rank 0
2024-02-15 15:53:43.193701: predicting ProstateOwn_129
2024-02-15 15:53:43.197050: ProstateOwn_129, shape torch.Size([1, 30, 308, 308]), rank 0
2024-02-15 15:53:45.699566: predicting ProstateOwn_130
2024-02-15 15:53:45.701476: ProstateOwn_130, shape torch.Size([1, 42, 308, 308]), rank 0
2024-02-15 15:53:49.437895: predicting ProstateOwn_137
2024-02-15 15:53:49.440661: ProstateOwn_137, shape torch.Size([1, 33, 308, 308]), rank 0
2024-02-15 15:53:51.940872: predicting ProstateOwn_151
2024-02-15 15:53:51.942859: ProstateOwn_151, shape torch.Size([1, 28, 308, 308]), rank 0
2024-02-15 15:53:54.438092: predicting ProstateOwn_156
2024-02-15 15:53:54.439660: ProstateOwn_156, shape torch.Size([1, 67, 411, 411]), rank 0
2024-02-15 15:54:08.432583: predicting ProstateOwn_20
2024-02-15 15:54:08.435287: ProstateOwn_20, shape torch.Size([1, 29, 308, 308]), rank 0
2024-02-15 15:54:10.934364: predicting ProstateOwn_24
2024-02-15 15:54:10.935645: ProstateOwn_24, shape torch.Size([1, 34, 307, 307]), rank 0
2024-02-15 15:54:13.430394: predicting ProstateOwn_3
2024-02-15 15:54:13.432062: ProstateOwn_3, shape torch.Size([1, 93, 359, 359]), rank 0
2024-02-15 15:54:22.161565: predicting ProstateOwn_33
2024-02-15 15:54:22.164835: ProstateOwn_33, shape torch.Size([1, 36, 334, 334]), rank 0
2024-02-15 15:54:24.666660: predicting ProstateOwn_39
2024-02-15 15:54:24.668218: ProstateOwn_39, shape torch.Size([1, 27, 308, 308]), rank 0
2024-02-15 15:54:27.162630: predicting ProstateOwn_42
2024-02-15 15:54:27.165126: ProstateOwn_42, shape torch.Size([1, 60, 359, 359]), rank 0
2024-02-15 15:54:32.164008: predicting ProstateOwn_46
2024-02-15 15:54:32.167267: ProstateOwn_46, shape torch.Size([1, 36, 308, 308]), rank 0
2024-02-15 15:54:34.667340: predicting ProstateOwn_53
2024-02-15 15:54:34.669094: ProstateOwn_53, shape torch.Size([1, 19, 257, 257]), rank 0
2024-02-15 15:54:35.927080: predicting ProstateOwn_54
2024-02-15 15:54:35.928092: ProstateOwn_54, shape torch.Size([1, 28, 308, 308]), rank 0
2024-02-15 15:54:38.423272: predicting ProstateOwn_58
2024-02-15 15:54:38.425290: ProstateOwn_58, shape torch.Size([1, 77, 359, 359]), rank 0
2024-02-15 15:54:45.907511: predicting ProstateOwn_59
2024-02-15 15:54:45.910532: ProstateOwn_59, shape torch.Size([1, 38, 308, 308]), rank 0
2024-02-15 15:54:49.649562: predicting ProstateOwn_63
2024-02-15 15:54:49.651645: ProstateOwn_63, shape torch.Size([1, 77, 385, 385]), rank 0
2024-02-15 15:55:06.430923: predicting ProstateOwn_65
2024-02-15 15:55:06.433714: ProstateOwn_65, shape torch.Size([1, 28, 308, 308]), rank 0
2024-02-15 15:55:08.932484: predicting ProstateOwn_68
2024-02-15 15:55:08.934918: ProstateOwn_68, shape torch.Size([1, 37, 308, 308]), rank 0
2024-02-15 15:55:12.669847: predicting ProstateOwn_7
2024-02-15 15:55:12.672172: ProstateOwn_7, shape torch.Size([1, 28, 308, 308]), rank 0
2024-02-15 15:55:15.165227: predicting ProstateOwn_76
2024-02-15 15:55:15.166840: ProstateOwn_76, shape torch.Size([1, 74, 449, 449]), rank 0
2024-02-15 15:55:31.959838: predicting ProstateOwn_8
2024-02-15 15:55:31.963764: ProstateOwn_8, shape torch.Size([1, 77, 449, 449]), rank 0
2024-02-15 15:55:48.755386: predicting ProstateOwn_88
2024-02-15 15:55:48.759327: ProstateOwn_88, shape torch.Size([1, 84, 488, 488]), rank 0
2024-02-15 15:56:05.580150: predicting ProstateOwn_92
2024-02-15 15:56:05.585428: ProstateOwn_92, shape torch.Size([1, 73, 359, 359]), rank 0
2024-02-15 15:56:13.070800: predicting ProstateOwn_94
2024-02-15 15:56:13.073788: ProstateOwn_94, shape torch.Size([1, 68, 385, 385]), rank 0
2024-02-15 15:56:27.064429: predicting ProstateOwn_99
2024-02-15 15:56:27.066894: ProstateOwn_99, shape torch.Size([1, 26, 308, 308]), rank 0
2024-02-15 15:57:02.074442: Validation complete
2024-02-15 15:57:02.074634: Mean Validation Dice:  0.895600397054955
